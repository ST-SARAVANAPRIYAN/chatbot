{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2619ae1d",
      "metadata": {
        "id": "2619ae1d"
      },
      "source": [
        "# Running Chatbot with Google Gemini on Colab\n",
        "\n",
        "This notebook runs the chatbot project using Google's Gemini API on Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9541b812",
      "metadata": {
        "id": "9541b812"
      },
      "source": [
        "## Step 1: Clone the GitHub repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "dae1e418",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dae1e418",
        "outputId": "d8f4b61c-85c7-401d-f2c4-43eb25a00c05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'chatbot'...\n",
            "remote: Enumerating objects: 38, done.\u001b[K\n",
            "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 38 (delta 6), reused 38 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (38/38), 40.08 KiB | 641.00 KiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n",
            "/content/chatbot\n"
          ]
        }
      ],
      "source": [
        "# Clone your GitHub repository\n",
        "!git clone https://github.com/ST-SARAVANAPRIYAN/chatbot.git\n",
        "%cd chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9e2fe7c",
      "metadata": {
        "id": "c9e2fe7c"
      },
      "source": [
        "## Step 2: Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3e692606",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3e692606",
        "outputId": "b403325e-d3a2-4a09-88b2-b7f162c6af6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index==0.9.8 in /usr/local/lib/python3.11/dist-packages (0.9.8)\n",
            "Requirement already satisfied: llama-index-core in /usr/local/lib/python3.11/dist-packages (0.12.46)\n",
            "Collecting llama-index-embeddings-gemini==0.1.0\n",
            "  Downloading llama_index_embeddings_gemini-0.1.0-py3-none-any.whl.metadata (646 bytes)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.46.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index==0.9.8) (2.0.41)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (3.11.15)\n",
            "Requirement already satisfied: aiostream<0.6.0,>=0.5.2 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (0.5.2)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (4.13.4)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (1.2.18)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (2025.3.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (1.6.0)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (2.0.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (1.93.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (4.14.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (0.9.0)\n",
            "Collecting google-generativeai<0.4.0,>=0.3.2 (from llama-index-embeddings-gemini==0.1.0)\n",
            "  Downloading google_generativeai-0.3.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting llama-index-core\n",
            "  Downloading llama_index_core-0.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (6.0.2)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (1.0.8)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (3.5)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (11.2.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (4.67.1)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.7)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.4.1)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.2)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.73.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (33.1.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.18)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.24.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (80.9.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.67)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.8) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.8) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.8) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.8) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.8) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.8) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.8) (1.20.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.45.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.2->llama-index==0.9.8) (2.7)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.9.3->llama-index==0.9.8) (1.17.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Collecting google-ai-generativelanguage==0.4.0 (from google-generativeai<0.4.0,>=0.3.2->llama-index-embeddings-gemini==0.1.0)\n",
            "  Downloading google_ai_generativelanguage-0.4.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.4.0,>=0.3.2->llama-index-embeddings-gemini==0.1.0) (2.38.0)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.4.0,>=0.3.2->llama-index-embeddings-gemini==0.1.0) (2.25.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.4.0->google-generativeai<0.4.0,>=0.3.2->llama-index-embeddings-gemini==0.1.0) (1.26.1)\n",
            "Collecting protobuf<7,>=3.20 (from streamlit)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index==0.9.8) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index==0.9.8) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index==0.9.8) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index==0.9.8) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index==0.9.8) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.26.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.8) (1.5.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index==0.9.8) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index==0.9.8) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index==0.9.8) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
            "INFO: pip is looking at multiple versions of opentelemetry-proto to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Using cached opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.34.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.34.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.34.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.34.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.33.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.33.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.33.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.33.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.33.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.33.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.33.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.33.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.33.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.33.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.33.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.32.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.32.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.32.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.32.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.32.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.32.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.32.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "INFO: pip is still looking at multiple versions of opentelemetry-proto to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.31.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.31.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.31.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.31.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.31.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.31.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.30.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.30.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.30.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.30.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.30.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.30.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.28.2 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.28.2 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.28.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.28.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.28.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.28.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.28.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.28.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.28.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.28.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.28.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.28.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting importlib-metadata<=8.4.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-8.4.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index==0.9.8) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index==0.9.8) (2025.2)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index==0.9.8) (3.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index==0.9.8) (3.2.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index==0.9.8) (1.1.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index==0.9.8) (3.26.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth->google-generativeai<0.4.0,>=0.3.2->llama-index-embeddings-gemini==0.1.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth->google-generativeai<0.4.0,>=0.3.2->llama-index-embeddings-gemini==0.1.0) (4.9.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.4.0->google-generativeai<0.4.0,>=0.3.2->llama-index-embeddings-gemini==0.1.0) (1.71.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth->google-generativeai<0.4.0,>=0.3.2->llama-index-embeddings-gemini==0.1.0) (0.6.1)\n",
            "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.4.0->google-generativeai<0.4.0,>=0.3.2->llama-index-embeddings-gemini==0.1.0)\n",
            "  Downloading grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.70.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.69.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading grpcio_status-1.68.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.68.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.67.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.67.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading grpcio_status-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.66.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.62.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "Downloading llama_index_embeddings_gemini-0.1.0-py3-none-any.whl (2.8 kB)\n",
            "Downloading llama_index_core-0.10.0-py3-none-any.whl (622 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.9/622.9 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_generativeai-0.3.2-py3-none-any.whl (146 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.9/146.9 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_ai_generativelanguage-0.4.0-py3-none-any.whl (598 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m598.7/598.7 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-8.4.0-py3-none-any.whl (26 kB)\n",
            "Downloading grpcio_status-1.62.3-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: protobuf, importlib-metadata, opentelemetry-proto, opentelemetry-api, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, grpcio-status, opentelemetry-sdk, llama-index-core, opentelemetry-exporter-otlp-proto-grpc, google-ai-generativelanguage, google-generativeai, llama-index-embeddings-gemini\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.7.0\n",
            "    Uninstalling importlib_metadata-8.7.0:\n",
            "      Successfully uninstalled importlib_metadata-8.7.0\n",
            "  Attempting uninstall: opentelemetry-proto\n",
            "    Found existing installation: opentelemetry-proto 1.34.1\n",
            "    Uninstalling opentelemetry-proto-1.34.1:\n",
            "      Successfully uninstalled opentelemetry-proto-1.34.1\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.34.1\n",
            "    Uninstalling opentelemetry-api-1.34.1:\n",
            "      Successfully uninstalled opentelemetry-api-1.34.1\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.55b1\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.55b1:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.55b1\n",
            "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
            "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.34.1\n",
            "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.34.1:\n",
            "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.34.1\n",
            "  Attempting uninstall: grpcio-status\n",
            "    Found existing installation: grpcio-status 1.71.2\n",
            "    Uninstalling grpcio-status-1.71.2:\n",
            "      Successfully uninstalled grpcio-status-1.71.2\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.34.1\n",
            "    Uninstalling opentelemetry-sdk-1.34.1:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.34.1\n",
            "  Attempting uninstall: llama-index-core\n",
            "    Found existing installation: llama-index-core 0.12.46\n",
            "    Uninstalling llama-index-core-0.12.46:\n",
            "      Successfully uninstalled llama-index-core-0.12.46\n",
            "  Attempting uninstall: opentelemetry-exporter-otlp-proto-grpc\n",
            "    Found existing installation: opentelemetry-exporter-otlp-proto-grpc 1.34.1\n",
            "    Uninstalling opentelemetry-exporter-otlp-proto-grpc-1.34.1:\n",
            "      Successfully uninstalled opentelemetry-exporter-otlp-proto-grpc-1.34.1\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "  Attempting uninstall: google-generativeai\n",
            "    Found existing installation: google-generativeai 0.8.5\n",
            "    Uninstalling google-generativeai-0.8.5:\n",
            "      Successfully uninstalled google-generativeai-0.8.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llama-index-embeddings-openai 0.3.1 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.10.0 which is incompatible.\n",
            "llama-index-cli 0.4.3 requires llama-index-core<0.13,>=0.12.0, but you have llama-index-core 0.10.0 which is incompatible.\n",
            "llama-index-multi-modal-llms-openai 0.5.1 requires llama-index-core<0.13,>=0.12.3, but you have llama-index-core 0.10.0 which is incompatible.\n",
            "llama-index-program-openai 0.3.2 requires llama-index-core<0.13,>=0.12.0, but you have llama-index-core 0.10.0 which is incompatible.\n",
            "llama-index-readers-llama-parse 0.4.0 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.10.0 which is incompatible.\n",
            "llama-index-readers-file 0.4.9 requires llama-index-core<0.13,>=0.12.0, but you have llama-index-core 0.10.0 which is incompatible.\n",
            "llama-index-indices-managed-llama-cloud 0.7.8 requires llama-index-core<0.13,>=0.12.0, but you have llama-index-core 0.10.0 which is incompatible.\n",
            "llama-index-agent-openai 0.4.12 requires llama-index-core<0.13,>=0.12.41, but you have llama-index-core 0.10.0 which is incompatible.\n",
            "llama-index-llms-openai 0.4.7 requires llama-index-core<0.13,>=0.12.41, but you have llama-index-core 0.10.0 which is incompatible.\n",
            "llama-index-question-gen-openai 0.3.1 requires llama-index-core<0.13,>=0.12.0, but you have llama-index-core 0.10.0 which is incompatible.\n",
            "llama-cloud-services 0.6.41 requires llama-index-core>=0.12.0, but you have llama-index-core 0.10.0 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-ai-generativelanguage-0.4.0 google-generativeai-0.3.2 grpcio-status-1.62.3 importlib-metadata-8.4.0 llama-index-core-0.10.0 llama-index-embeddings-gemini-0.1.0 opentelemetry-api-1.27.0 opentelemetry-exporter-otlp-proto-common-1.27.0 opentelemetry-exporter-otlp-proto-grpc-1.27.0 opentelemetry-proto-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 protobuf-4.25.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "importlib_metadata"
                ]
              },
              "id": "bc3c87e93b5545c5b1057360a6cdac8c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
            "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/__init__.py\", line 6, in <module>\n",
            "    from .errors import setup_default_warnings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/errors.py\", line 3, in <module>\n",
            "    from .compat import Literal\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/compat.py\", line 4, in <module>\n",
            "    from thinc.util import copy_array\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/thinc/__init__.py\", line 5, in <module>\n",
            "    from .config import registry\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/thinc/config.py\", line 5, in <module>\n",
            "    from .types import Decorator\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/thinc/types.py\", line 27, in <module>\n",
            "    from .compat import cupy, has_cupy\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/thinc/compat.py\", line 35, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 2604, in <module>\n",
            "    from torch import _meta_registrations\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_meta_registrations.py\", line 11, in <module>\n",
            "    from torch._decomp import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_decomp/__init__.py\", line 284, in <module>\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install llama-index==0.9.8 llama-index-core llama-index-embeddings-gemini==0.1.0 chromadb python-dotenv streamlit spacy transformers torch langchain\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2276b2e7",
      "metadata": {
        "id": "2276b2e7"
      },
      "source": [
        "## Step 3: Set up your Gemini API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "81956d90",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81956d90",
        "outputId": "4927c18f-aeeb-4259-e2a2-b8e30b4bdb85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Gemini API key: ··········\n"
          ]
        }
      ],
      "source": [
        "# Set up your Gemini API key\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Securely input your API key\n",
        "GEMINI_API_KEY = getpass('Enter your Gemini API key: ')\n",
        "os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY\n",
        "\n",
        "# Create .env file with the API key\n",
        "with open('.env', 'w') as f:\n",
        "    f.write(f\"GEMINI_API_KEY={GEMINI_API_KEY}\\n\")\n",
        "    f.write(\"CHROMA_DB_DIRECTORY=./chroma_db\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5825b5b3",
      "metadata": {
        "id": "5825b5b3"
      },
      "source": [
        "## Step 4: Create a version-flexible chatbot implementation\n",
        "\n",
        "This version handles different import structures in different versions of LlamaIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f6bdcec8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6bdcec8",
        "outputId": "10b0a330-1703-4173-9dfa-0d29c095a7de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing colab_chatbot_flexible.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile colab_chatbot_flexible.py\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "import google.generativeai as genai\n",
        "from dotenv import load_dotenv\n",
        "import chromadb\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Directory paths\n",
        "DATA_DIR = \"./data\"\n",
        "CHROMA_DB_DIRECTORY = \"./chroma_db\"\n",
        "\n",
        "class GeminiChatbot:\n",
        "    def __init__(self):\n",
        "        # Get Gemini API key\n",
        "        self.api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            print(\"GEMINI_API_KEY not found in environment. Please set it first.\")\n",
        "            self.api_key = input(\"Enter your Gemini API key: \")\n",
        "            os.environ[\"GEMINI_API_KEY\"] = self.api_key\n",
        "\n",
        "        # Configure Gemini\n",
        "        genai.configure(api_key=self.api_key)\n",
        "        self.model = genai.GenerativeModel('gemini-pro')\n",
        "        self.index = None\n",
        "\n",
        "    def load_documents(self):\n",
        "        \"\"\"Load documents from the data directory\"\"\"\n",
        "        if not os.path.exists(DATA_DIR):\n",
        "            os.makedirs(DATA_DIR)\n",
        "            print(f\"Created data directory at {DATA_DIR}\")\n",
        "\n",
        "        if not os.listdir(DATA_DIR):\n",
        "            print(\"Data directory is empty. Please add some documents.\")\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def build_index(self):\n",
        "        \"\"\"Build vector index from documents\"\"\"\n",
        "        try:\n",
        "            # Dynamic imports to handle different versions of llama-index\n",
        "            try:\n",
        "                # Try importing from llama_index (newer versions)\n",
        "                from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
        "                from llama_index.node_parser import SentenceSplitter\n",
        "                from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "                print(\"Using llama_index package\")\n",
        "            except ImportError:\n",
        "                # Try importing from llama_index.core (older or different versions)\n",
        "                from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
        "                from llama_index.core.node_parser import SentenceSplitter\n",
        "                try:\n",
        "                    from llama_index.core.vector_stores.chroma import ChromaVectorStore\n",
        "                    print(\"Using llama_index.core package\")\n",
        "                except ImportError:\n",
        "                    from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "                    print(\"Using mixed llama_index imports\")\n",
        "\n",
        "            # Load documents\n",
        "            documents = SimpleDirectoryReader(DATA_DIR).load_data()\n",
        "\n",
        "            # Create sentence splitter for text chunking\n",
        "            text_splitter = SentenceSplitter(\n",
        "                chunk_size=1000,\n",
        "                chunk_overlap=200\n",
        "            )\n",
        "\n",
        "            # Set up Chroma client\n",
        "            if not os.path.exists(CHROMA_DB_DIRECTORY):\n",
        "                os.makedirs(CHROMA_DB_DIRECTORY)\n",
        "\n",
        "            chroma_client = chromadb.PersistentClient(path=CHROMA_DB_DIRECTORY)\n",
        "            chroma_collection = chroma_client.get_or_create_collection(\"documents\")\n",
        "            vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "            storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "            # Build index\n",
        "            try:\n",
        "                self.index = VectorStoreIndex.from_documents(\n",
        "                    documents,\n",
        "                    storage_context=storage_context,\n",
        "                    transformations=[text_splitter],\n",
        "                )\n",
        "            except TypeError:\n",
        "                # Try alternative approach if transformations parameter doesn't work\n",
        "                self.index = VectorStoreIndex.from_documents(\n",
        "                    documents,\n",
        "                    storage_context=storage_context,\n",
        "                    node_parser=text_splitter,\n",
        "                )\n",
        "\n",
        "            print(\"Index built successfully!\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error building index: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return False\n",
        "\n",
        "    def query(self, query_text):\n",
        "        \"\"\"Query the index with a natural language query\"\"\"\n",
        "        if not self.index:\n",
        "            print(\"Index not built yet. Please build the index first.\")\n",
        "            return {\n",
        "                \"answer\": \"Index not built yet. Please build the index first.\",\n",
        "                \"sources\": []\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            # Create query engine\n",
        "            query_engine = self.index.as_query_engine()\n",
        "\n",
        "            # Execute query\n",
        "            response = query_engine.query(query_text)\n",
        "\n",
        "            # Extract source information\n",
        "            sources = []\n",
        "            if hasattr(response, 'source_nodes'):\n",
        "                for node in response.source_nodes:\n",
        "                    source = {\n",
        "                        \"text\": node.node.get_content(),\n",
        "                        \"metadata\": node.node.metadata,\n",
        "                        \"score\": node.score if hasattr(node, 'score') else None\n",
        "                    }\n",
        "                    sources.append(source)\n",
        "\n",
        "            return {\n",
        "                \"answer\": str(response),\n",
        "                \"sources\": sources\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error during query: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return {\n",
        "                \"answer\": f\"Error during query: {str(e)}\",\n",
        "                \"sources\": []\n",
        "            }\n",
        "\n",
        "    def chat_interface(self):\n",
        "        \"\"\"Simple chat interface for testing\"\"\"\n",
        "        print(\"\\n===== Gemini Chatbot =====\")\n",
        "        print(\"Type 'exit' to quit\")\n",
        "        print(\"========================\\n\")\n",
        "\n",
        "        while True:\n",
        "            query = input(\"\\nYou: \")\n",
        "            if query.lower() in ['exit', 'quit', 'q']:\n",
        "                break\n",
        "\n",
        "            response = self.query(query)\n",
        "            print(f\"\\nBot: {response['answer']}\")\n",
        "\n",
        "            # Print sources\n",
        "            if response['sources']:\n",
        "                print(\"\\nSources:\")\n",
        "                for i, source in enumerate(response['sources'][:2], 1):\n",
        "                    source_name = source.get('metadata', {}).get('source', f\"Source {i}\")\n",
        "                    print(f\"- {source_name}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Initializing chatbot...\")\n",
        "    chatbot = GeminiChatbot()\n",
        "\n",
        "    print(\"Loading documents...\")\n",
        "    if chatbot.load_documents():\n",
        "        print(\"Building index...\")\n",
        "        if chatbot.build_index():\n",
        "            print(\"Ready to chat!\")\n",
        "            chatbot.chat_interface()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1520bf54",
      "metadata": {
        "id": "1520bf54"
      },
      "source": [
        "## Step 5: Upload custom content (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f02b7662",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "f02b7662",
        "outputId": "a43cf373-d34c-4337-f610-4d4a2fa5b41a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your content files (optional). Skip this if you want to use the sample FAQ.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-00df729a-3be9-4c4a-88d0-b5e5b429841f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-00df729a-3be9-4c4a-88d0-b5e5b429841f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Upload your own content files (optional)\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Create data directory if it doesn't exist\n",
        "if not os.path.exists('data'):\n",
        "    os.makedirs('data')\n",
        "\n",
        "# Let user upload files\n",
        "print(\"Upload your content files (optional). Skip this if you want to use the sample FAQ.\")\n",
        "try:\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Save uploaded files to the data directory\n",
        "    for filename, content in uploaded.items():\n",
        "        with open(f'data/{filename}', 'wb') as f:\n",
        "            f.write(content)\n",
        "        print(f\"Saved {filename} to data directory\")\n",
        "except Exception as e:\n",
        "    print(f\"No files uploaded or error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15f465dc",
      "metadata": {
        "id": "15f465dc"
      },
      "source": [
        "## Step 6: Run the chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "12a09518",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12a09518",
        "outputId": "e4958cb3-39b1-4448-e410-84e9a68ca7b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing chatbot...\n",
            "Loading documents...\n",
            "Building index...\n",
            "Attempting to import LlamaIndex components...\n",
            "Failed to import VectorStoreIndex from any of the provided paths: ['llama_index', 'llama_index.core', 'llama_index.indices.vector_store', 'llama_index.core.indices.vector_store']\n",
            "Failed to import SimpleDirectoryReader from any of the provided paths: ['llama_index', 'llama_index.core', 'llama_index.readers', 'llama_index.core.readers']\n",
            "Failed to import StorageContext from any of the provided paths: ['llama_index', 'llama_index.core', 'llama_index.storage', 'llama_index.core.storage']\n",
            "Failed to import SentenceSplitter from any of the provided paths: ['llama_index.node_parser', 'llama_index.core.node_parser', 'llama_index.text_splitter', 'llama_index.core.text_splitter']\n",
            "Failed to import required components: ['VectorStoreIndex', 'SimpleDirectoryReader', 'StorageContext', 'SentenceSplitter', 'ChromaVectorStore']\n",
            "Attempting to install llama-index...\n",
            "Requirement already satisfied: llama-index==0.9.8 in /usr/local/lib/python3.11/dist-packages (0.9.8)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index==0.9.8) (2.0.41)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (3.11.15)\n",
            "Requirement already satisfied: aiostream<0.6.0,>=0.5.2 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (0.5.2)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (4.13.4)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (1.2.18)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (2025.3.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (1.6.0)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (2.0.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (1.93.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (4.14.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.9.8) (0.9.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.8) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.8) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.8) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.8) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.8) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.8) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.8) (1.20.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.2->llama-index==0.9.8) (2.7)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.9.3->llama-index==0.9.8) (1.17.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.8) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.8) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.8) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.8) (4.67.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index==0.9.8) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index==0.9.8) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index==0.9.8) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index==0.9.8) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index==0.9.8) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index==0.9.8) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index==0.9.8) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index==0.9.8) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index==0.9.8) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index==0.9.8) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index==0.9.8) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index==0.9.8) (3.2.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index==0.9.8) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index==0.9.8) (3.26.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index==0.9.8) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index==0.9.8) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index==0.9.8) (2025.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index==0.9.8) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index==0.9.8) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index==0.9.8) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index==0.9.8) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index==0.9.8) (1.17.0)\n",
            "Please restart the notebook and try again.\n"
          ]
        }
      ],
      "source": [
        "# Run the command-line chatbot\n",
        "!python colab_chatbot_flexible.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample document in the data directory\n",
        "!mkdir -p data\n",
        "%%writefile data/sample_faq.md\n",
        "# Sample FAQ\n",
        "\n",
        "## What is this chatbot?\n",
        "This is a RAG-based chatbot using Google Gemini API.\n",
        "\n",
        "## What is RAG?\n",
        "RAG stands for Retrieval-Augmented Generation, which enhances LLM responses with retrieved information.\n",
        "\n",
        "## How does it work?\n",
        "It uses vector embeddings to find relevant information and then generates responses based on that information."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "iCKhYdNGMT6B",
        "outputId": "40af544c-e99a-45a3-f626-f9d3db7acd34"
      },
      "id": "iCKhYdNGMT6B",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-12-4085606628.py, line 7)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-12-4085606628.py\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    This is a RAG-based chatbot using Google Gemini API.\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the chatbot in the same Colab notebook\n",
        "%run colab_chatbot_flexible.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cgpw0Ep7MtCs",
        "outputId": "81a66751-5a58-4f87-bd1c-d24a9d6811b3"
      },
      "id": "Cgpw0Ep7MtCs",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing chatbot...\n",
            "Loading documents...\n",
            "Building index...\n",
            "Attempting to import LlamaIndex components...\n",
            "Failed to import required components: ['ChromaVectorStore']\n",
            "Attempting to install llama-index...\n",
            "Please restart the notebook and try again.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PvlxiDFfPusn"
      },
      "id": "PvlxiDFfPusn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Try to get the API key from Colab secrets\n",
        "try:\n",
        "  GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "except:\n",
        "  # If not in secrets, ask user for it\n",
        "  GEMINI_API_KEY = input('Enter your Gemini API key: ')\n",
        "\n",
        "os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY\n",
        "\n",
        "# Create .env file with the API key\n",
        "with open('.env', 'w') as f:\n",
        "    f.write(f\"GEMINI_API_KEY={GEMINI_API_KEY}\\n\")\n",
        "\n",
        "print(\"API key set successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NS_o-mQTMgCS",
        "outputId": "13aba296-148c-4d13-c040-e2554c3aeda3"
      },
      "id": "NS_o-mQTMgCS",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Gemini API key:  AIzaSyD7FHAOs8JPMfhB_mmTOPy5rMhufMW2450\n",
            "API key set successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b217d15c",
      "metadata": {
        "id": "b217d15c"
      },
      "source": [
        "## Step 7: Create Streamlit Interface (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d010df8f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d010df8f",
        "outputId": "144acb71-8e52-4659-8288-1be7c0969396"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing colab_streamlit_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile colab_streamlit_app.py\n",
        "import streamlit as st\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "from dotenv import load_dotenv\n",
        "import chromadb\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Configure Gemini\n",
        "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "if api_key:\n",
        "    genai.configure(api_key=api_key)\n",
        "\n",
        "# Directory paths\n",
        "DATA_DIR = \"./data\"\n",
        "CHROMA_DB_DIRECTORY = \"./chroma_db\"\n",
        "\n",
        "# Initialize session state\n",
        "if 'index' not in st.session_state:\n",
        "    st.session_state.index = None\n",
        "if 'messages' not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "def build_index():\n",
        "    \"\"\"Build or load the vector index\"\"\"\n",
        "    try:\n",
        "        # Dynamic imports to handle different versions of llama-index\n",
        "        try:\n",
        "            # Try importing from llama_index (newer versions)\n",
        "            from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
        "            from llama_index.node_parser import SentenceSplitter\n",
        "            from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "            st.info(\"Using llama_index package\")\n",
        "        except ImportError:\n",
        "            # Try importing from llama_index.core (older or different versions)\n",
        "            from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
        "            from llama_index.core.node_parser import SentenceSplitter\n",
        "            try:\n",
        "                from llama_index.core.vector_stores.chroma import ChromaVectorStore\n",
        "                st.info(\"Using llama_index.core package\")\n",
        "            except ImportError:\n",
        "                from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "                st.info(\"Using mixed llama_index imports\")\n",
        "\n",
        "        if not os.path.exists(DATA_DIR):\n",
        "            os.makedirs(DATA_DIR)\n",
        "\n",
        "        # Load documents\n",
        "        documents = SimpleDirectoryReader(DATA_DIR).load_data()\n",
        "\n",
        "        # Create sentence splitter for text chunking\n",
        "        text_splitter = SentenceSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200\n",
        "        )\n",
        "\n",
        "        # Set up Chroma client\n",
        "        if not os.path.exists(CHROMA_DB_DIRECTORY):\n",
        "            os.makedirs(CHROMA_DB_DIRECTORY)\n",
        "\n",
        "        chroma_client = chromadb.PersistentClient(path=CHROMA_DB_DIRECTORY)\n",
        "        chroma_collection = chroma_client.get_or_create_collection(\"documents\")\n",
        "        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "        # Build index\n",
        "        try:\n",
        "            index = VectorStoreIndex.from_documents(\n",
        "                documents,\n",
        "                storage_context=storage_context,\n",
        "                transformations=[text_splitter],\n",
        "            )\n",
        "        except TypeError:\n",
        "            # Try alternative approach if transformations parameter doesn't work\n",
        "            index = VectorStoreIndex.from_documents(\n",
        "                documents,\n",
        "                storage_context=storage_context,\n",
        "                node_parser=text_splitter,\n",
        "            )\n",
        "\n",
        "        return index\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error building index: {str(e)}\")\n",
        "        import traceback\n",
        "        st.error(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "def query_index(query_text):\n",
        "    \"\"\"Query the index with a natural language query\"\"\"\n",
        "    if not st.session_state.index:\n",
        "        return {\n",
        "            \"answer\": \"Index not built yet. Please build the index first.\",\n",
        "            \"sources\": []\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        # Create query engine\n",
        "        query_engine = st.session_state.index.as_query_engine()\n",
        "\n",
        "        # Execute query\n",
        "        response = query_engine.query(query_text)\n",
        "\n",
        "        # Extract source information\n",
        "        sources = []\n",
        "        if hasattr(response, 'source_nodes'):\n",
        "            for node in response.source_nodes:\n",
        "                source = {\n",
        "                    \"text\": node.node.get_content(),\n",
        "                    \"metadata\": node.node.metadata,\n",
        "                    \"score\": node.score if hasattr(node, 'score') else None\n",
        "                }\n",
        "                sources.append(source)\n",
        "\n",
        "        return {\n",
        "            \"answer\": str(response),\n",
        "            \"sources\": sources\n",
        "        }\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        st.error(traceback.format_exc())\n",
        "        return {\n",
        "            \"answer\": f\"Error during query: {str(e)}\",\n",
        "            \"sources\": []\n",
        "        }\n",
        "\n",
        "# App title\n",
        "st.title(\"🤖 Gemini Chatbot\")\n",
        "\n",
        "# Sidebar\n",
        "with st.sidebar:\n",
        "    st.title(\"Settings\")\n",
        "\n",
        "    # API key input\n",
        "    if not api_key:\n",
        "        new_api_key = st.text_input(\"Enter Gemini API Key\", type=\"password\")\n",
        "        if new_api_key:\n",
        "            genai.configure(api_key=new_api_key)\n",
        "            os.environ[\"GEMINI_API_KEY\"] = new_api_key\n",
        "            with open('.env', 'w') as f:\n",
        "                f.write(f\"GEMINI_API_KEY={new_api_key}\\n\")\n",
        "                f.write(\"CHROMA_DB_DIRECTORY=./chroma_db\\n\")\n",
        "            st.success(\"API key set!\")\n",
        "            api_key = new_api_key\n",
        "\n",
        "    # Build index button\n",
        "    if st.button(\"Build/Rebuild Index\"):\n",
        "        with st.spinner(\"Building index...\"):\n",
        "            st.session_state.index = build_index()\n",
        "            if st.session_state.index:\n",
        "                st.success(\"Index built successfully!\")\n",
        "            else:\n",
        "                st.error(\"Failed to build index\")\n",
        "\n",
        "    # Show data directory contents\n",
        "    st.subheader(\"Data Directory\")\n",
        "    if os.path.exists(DATA_DIR):\n",
        "        files = os.listdir(DATA_DIR)\n",
        "        if files:\n",
        "            st.write(f\"Found {len(files)} files:\")\n",
        "            for file in files:\n",
        "                st.write(f\"- {file}\")\n",
        "        else:\n",
        "            st.write(\"No files found in data directory\")\n",
        "    else:\n",
        "        st.write(\"Data directory does not exist\")\n",
        "\n",
        "    # Clear chat history\n",
        "    if st.button(\"Clear Chat History\"):\n",
        "        st.session_state.messages = []\n",
        "        st.success(\"Chat history cleared\")\n",
        "\n",
        "# Initialize or load index\n",
        "if st.session_state.index is None:\n",
        "    if os.path.exists(CHROMA_DB_DIRECTORY):\n",
        "        with st.spinner(\"Loading existing index...\"):\n",
        "            st.session_state.index = build_index()\n",
        "            if st.session_state.index:\n",
        "                st.success(\"Index loaded successfully!\")\n",
        "    else:\n",
        "        st.info(\"No index found. Please build the index using the sidebar button.\")\n",
        "\n",
        "# Display chat messages\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "        # Show sources if available\n",
        "        if message[\"role\"] == \"assistant\" and \"sources\" in message:\n",
        "            with st.expander(\"View Sources\"):\n",
        "                for i, source in enumerate(message[\"sources\"], 1):\n",
        "                    source_name = source.get('metadata', {}).get('source', f\"Source {i}\")\n",
        "                    st.markdown(f\"**{source_name}**\")\n",
        "                    text = source.get('text', '')\n",
        "                    if text:\n",
        "                        st.text(text[:200] + \"...\" if len(text) > 200 else text)\n",
        "\n",
        "# Chat input\n",
        "if prompt := st.chat_input(\"Ask me anything...\"):\n",
        "    # Check if API key is set\n",
        "    if not api_key:\n",
        "        st.error(\"Please set your Gemini API key in the sidebar first.\")\n",
        "        st.stop()\n",
        "\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    # Display user message\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    # Generate and display assistant response\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            if st.session_state.index is None:\n",
        "                response_content = \"Please build the index first using the sidebar button.\"\n",
        "                st.markdown(response_content)\n",
        "                st.session_state.messages.append({\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": response_content\n",
        "                })\n",
        "            else:\n",
        "                try:\n",
        "                    response = query_index(prompt)\n",
        "                    st.markdown(response[\"answer\"])\n",
        "\n",
        "                    # Add assistant message to chat history\n",
        "                    st.session_state.messages.append({\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": response[\"answer\"],\n",
        "                        \"sources\": response[\"sources\"]\n",
        "                    })\n",
        "\n",
        "                    # Show sources if available\n",
        "                    if response[\"sources\"]:\n",
        "                        with st.expander(\"View Sources\"):\n",
        "                            for i, source in enumerate(response[\"sources\"], 1):\n",
        "                                source_name = source.get('metadata', {}).get('source', f\"Source {i}\")\n",
        "                                st.markdown(f\"**{source_name}**\")\n",
        "                                text = source.get('text', '')\n",
        "                                if text:\n",
        "                                    st.text(text[:200] + \"...\" if len(text) > 200 else text)\n",
        "                except Exception as e:\n",
        "                    error_msg = f\"Error: {str(e)}\"\n",
        "                    st.error(error_msg)\n",
        "                    st.session_state.messages.append({\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": error_msg\n",
        "                    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c61ea93",
      "metadata": {
        "id": "1c61ea93"
      },
      "source": [
        "## Step 8: Create the Streamlit interface for Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "037a342d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "037a342d",
        "outputId": "d6e6ee99-fa71-4905-9a5c-08db66f277b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting colab_streamlit_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile colab_streamlit_app.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Streamlit web interface for the chatbot - Google Colab version\n",
        "\"\"\"\n",
        "import streamlit as st\n",
        "import logging\n",
        "import sys\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "from dotenv import load_dotenv\n",
        "import chromadb\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Directory paths\n",
        "DATA_DIR = \"./data\"\n",
        "CHROMA_DB_DIRECTORY = \"./chroma_db\"\n",
        "\n",
        "class GeminiChatbot:\n",
        "    def __init__(self):\n",
        "        # Get Gemini API key\n",
        "        self.api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            print(\"GEMINI_API_KEY not found in environment. Please set it first.\")\n",
        "            self.api_key = st.text_input(\"Enter your Gemini API key:\", type=\"password\")\n",
        "            if not self.api_key:\n",
        "                st.error(\"Gemini API key is required to continue.\")\n",
        "                st.stop()\n",
        "            os.environ[\"GEMINI_API_KEY\"] = self.api_key\n",
        "\n",
        "        # Configure Gemini\n",
        "        genai.configure(api_key=self.api_key)\n",
        "        self.model = genai.GenerativeModel('gemini-pro')\n",
        "        self.index = None\n",
        "\n",
        "    def load_documents(self):\n",
        "        \"\"\"Load documents from the data directory\"\"\"\n",
        "        if not os.path.exists(DATA_DIR):\n",
        "            os.makedirs(DATA_DIR)\n",
        "            st.info(f\"Created data directory at {DATA_DIR}\")\n",
        "\n",
        "        if not os.listdir(DATA_DIR):\n",
        "            st.warning(\"Data directory is empty. Please add some documents.\")\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def build_index(self):\n",
        "        \"\"\"Build vector index from documents\"\"\"\n",
        "        try:\n",
        "            # Dynamic imports to handle different versions of llama-index\n",
        "            try:\n",
        "                # Try importing from llama_index (newer versions)\n",
        "                from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
        "                from llama_index.node_parser import SentenceSplitter\n",
        "                from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "                st.info(\"Using llama_index package\")\n",
        "            except ImportError:\n",
        "                # Try importing from llama_index.core (older or different versions)\n",
        "                from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
        "                from llama_index.core.node_parser import SentenceSplitter\n",
        "                try:\n",
        "                    from llama_index.core.vector_stores.chroma import ChromaVectorStore\n",
        "                    st.info(\"Using llama_index.core package\")\n",
        "                except ImportError:\n",
        "                    from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "                    st.info(\"Using mixed llama_index imports\")\n",
        "\n",
        "            # Load documents\n",
        "            documents = SimpleDirectoryReader(DATA_DIR).load_data()\n",
        "\n",
        "            # Create sentence splitter for text chunking\n",
        "            text_splitter = SentenceSplitter(\n",
        "                chunk_size=1000,\n",
        "                chunk_overlap=200\n",
        "            )\n",
        "\n",
        "            # Set up Chroma client\n",
        "            if not os.path.exists(CHROMA_DB_DIRECTORY):\n",
        "                os.makedirs(CHROMA_DB_DIRECTORY)\n",
        "\n",
        "            chroma_client = chromadb.PersistentClient(path=CHROMA_DB_DIRECTORY)\n",
        "            chroma_collection = chroma_client.get_or_create_collection(\"documents\")\n",
        "            vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "            storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "            # Build index with error handling for different parameter names\n",
        "            try:\n",
        "                self.index = VectorStoreIndex.from_documents(\n",
        "                    documents,\n",
        "                    storage_context=storage_context,\n",
        "                    transformations=[text_splitter],\n",
        "                )\n",
        "            except TypeError:\n",
        "                # Try alternative approach if transformations parameter doesn't work\n",
        "                self.index = VectorStoreIndex.from_documents(\n",
        "                    documents,\n",
        "                    storage_context=storage_context,\n",
        "                    node_parser=text_splitter,\n",
        "                )\n",
        "\n",
        "            st.success(\"Index built successfully!\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error building index: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return False\n",
        "\n",
        "    def query(self, query_text):\n",
        "        \"\"\"Query the index with a natural language query\"\"\"\n",
        "        if not self.index:\n",
        "            st.error(\"Index not built yet. Please build the index first.\")\n",
        "            return {\n",
        "                \"answer\": \"Index not built yet. Please build the index first.\",\n",
        "                \"sources\": []\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            # Create query engine\n",
        "            query_engine = self.index.as_query_engine()\n",
        "\n",
        "            # Execute query\n",
        "            response = query_engine.query(query_text)\n",
        "\n",
        "            # Extract source information\n",
        "            sources = []\n",
        "            if hasattr(response, 'source_nodes'):\n",
        "                for node in response.source_nodes:\n",
        "                    source = {\n",
        "                        \"text\": node.node.get_content(),\n",
        "                        \"metadata\": node.node.metadata,\n",
        "                        \"score\": node.score if hasattr(node, 'score') else None\n",
        "                    }\n",
        "                    sources.append(source)\n",
        "\n",
        "            return {\n",
        "                \"answer\": str(response),\n",
        "                \"sources\": sources\n",
        "            }\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error during query: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return {\n",
        "                \"answer\": f\"Error during query: {str(e)}\",\n",
        "                \"sources\": []\n",
        "            }\n",
        "\n",
        "def init_session_state():\n",
        "    \"\"\"Initialize session state variables\"\"\"\n",
        "    if 'messages' not in st.session_state:\n",
        "        st.session_state.messages = []\n",
        "    if 'chatbot' not in st.session_state:\n",
        "        st.session_state.chatbot = None\n",
        "\n",
        "def main():\n",
        "    # Set page config\n",
        "    st.set_page_config(\n",
        "        page_title=\"Gemini Chatbot for Google Colab\",\n",
        "        page_icon=\"🤖\",\n",
        "        layout=\"wide\"\n",
        "    )\n",
        "\n",
        "    # Initialize session state\n",
        "    init_session_state()\n",
        "\n",
        "    # Main title\n",
        "    st.title(\"🤖 Gemini Chatbot for Google Colab\")\n",
        "\n",
        "    # Sidebar\n",
        "    with st.sidebar:\n",
        "        st.title(\"Chatbot Settings\")\n",
        "\n",
        "        # Initialize chatbot if not already done\n",
        "        if st.session_state.chatbot is None:\n",
        "            st.session_state.chatbot = GeminiChatbot()\n",
        "\n",
        "        # Check data directory status\n",
        "        data_dir_status = \"✅ Available\" if os.path.exists(DATA_DIR) and os.listdir(DATA_DIR) else \"❌ Empty\"\n",
        "        st.info(f\"Data Directory: {data_dir_status}\")\n",
        "\n",
        "        # Check index status\n",
        "        index_status = \"✅ Available\" if os.path.exists(CHROMA_DB_DIRECTORY) else \"❌ Not Built\"\n",
        "        st.info(f\"Vector Index: {index_status}\")\n",
        "\n",
        "        # Upload documents\n",
        "        with st.expander(\"Upload Documents\"):\n",
        "            uploaded_file = st.file_uploader(\"Upload a document\", type=[\"txt\", \"md\", \"pdf\"])\n",
        "            if uploaded_file is not None:\n",
        "                if not os.path.exists(DATA_DIR):\n",
        "                    os.makedirs(DATA_DIR)\n",
        "\n",
        "                file_path = os.path.join(DATA_DIR, uploaded_file.name)\n",
        "                with open(file_path, \"wb\") as f:\n",
        "                    f.write(uploaded_file.getvalue())\n",
        "                st.success(f\"File '{uploaded_file.name}' uploaded successfully\")\n",
        "\n",
        "        # Build index button\n",
        "        if st.button(\"Build/Rebuild Vector Index\"):\n",
        "            if st.session_state.chatbot.load_documents():\n",
        "                with st.spinner(\"Building vector index...\"):\n",
        "                    if st.session_state.chatbot.build_index():\n",
        "                        st.success(\"Vector index built successfully!\")\n",
        "                    else:\n",
        "                        st.error(\"Failed to build vector index.\")\n",
        "            else:\n",
        "                st.error(\"No documents found. Please upload some documents first.\")\n",
        "\n",
        "    # Display chat messages\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.markdown(message[\"content\"])\n",
        "\n",
        "            # Show sources if available\n",
        "            if message[\"role\"] == \"assistant\" and \"sources\" in message and message[\"sources\"]:\n",
        "                with st.expander(\"View Sources\"):\n",
        "                    for i, source in enumerate(message[\"sources\"], 1):\n",
        "                        source_name = source.get('metadata', {}).get('source', f\"Source {i}\")\n",
        "                        st.markdown(f\"**Document: {source_name}**\")\n",
        "                        text = source.get('text', '')\n",
        "                        if text:\n",
        "                            st.text(text[:200] + \"...\" if len(text) > 200 else text)\n",
        "\n",
        "    # Chat input\n",
        "    if prompt := st.chat_input(\"Ask me anything...\"):\n",
        "        # Add user message to chat history\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "        # Display user message\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.markdown(prompt)\n",
        "\n",
        "        # Check if index is built\n",
        "        if not st.session_state.chatbot.index:\n",
        "            if not os.path.exists(CHROMA_DB_DIRECTORY) or not os.listdir(CHROMA_DB_DIRECTORY):\n",
        "                # Need to build index first\n",
        "                if st.session_state.chatbot.load_documents():\n",
        "                    with st.spinner(\"Building index for the first time...\"):\n",
        "                        st.session_state.chatbot.build_index()\n",
        "                else:\n",
        "                    st.error(\"No documents found. Please upload some documents first.\")\n",
        "                    st.session_state.messages.append({\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": \"I need some documents to learn from. Please upload documents using the sidebar.\",\n",
        "                        \"sources\": []\n",
        "                    })\n",
        "                    st.stop()\n",
        "            else:\n",
        "                # Index exists but not loaded\n",
        "                with st.spinner(\"Loading existing index...\"):\n",
        "                    st.session_state.chatbot.build_index()\n",
        "\n",
        "        # Generate and display assistant response\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            message_placeholder = st.empty()\n",
        "            with st.spinner(\"Thinking...\"):\n",
        "                response = st.session_state.chatbot.query(prompt)\n",
        "                message_placeholder.markdown(response[\"answer\"])\n",
        "\n",
        "                # Add assistant message to chat history\n",
        "                st.session_state.messages.append({\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": response[\"answer\"],\n",
        "                    \"sources\": response[\"sources\"]\n",
        "                })\n",
        "\n",
        "                # Show sources if available\n",
        "                if response[\"sources\"]:\n",
        "                    with st.expander(\"View Sources\"):\n",
        "                        for i, source in enumerate(response[\"sources\"], 1):\n",
        "                            source_name = source.get('metadata', {}).get('source', f\"Source {i}\")\n",
        "                            st.markdown(f\"**Document: {source_name}**\")\n",
        "                            text = source.get('text', '')\n",
        "                            if text:\n",
        "                                st.text(text[:200] + \"...\" if len(text) > 200 else text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf55f626",
      "metadata": {
        "id": "bf55f626"
      },
      "source": [
        "## Step 9: Run Streamlit Interface (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "52d5a9cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "52d5a9cb",
        "outputId": "86d4d034-0a16-459e-c141-8e9360b1428b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.11-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.11-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.11\n",
            "Enter your ngrok auth token (optional, press Enter to skip): \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pyngrok.process.ngrok:t=2025-07-04T16:03:20+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokError",
          "evalue": "The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-2044893009.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Start ngrok tunnel to Streamlit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8501\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n\\nStreamlit app is available at: {public_url}\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Opening tunnel named: {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m     \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngrok_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Creating tunnel with options: {options}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0minstall_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_current_processes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrok_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_start_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartup_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             raise PyngrokNgrokError(f\"The ngrok process errored on start: {ngrok_process.startup_error}.\",\n\u001b[0m\u001b[1;32m    448\u001b[0m                                     \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m                                     ngrok_process.startup_error)\n",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n."
          ]
        }
      ],
      "source": [
        "# Install ngrok and pyngrok if you want to run the Streamlit app\n",
        "!pip install pyngrok\n",
        "\n",
        "# Run Streamlit app using ngrok\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "\n",
        "# Set up ngrok authentication (optional, needed for better reliability)\n",
        "# Get your token from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "NGROK_AUTH_TOKEN = input(\"Enter your ngrok auth token (optional, press Enter to skip): \")\n",
        "if NGROK_AUTH_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Start ngrok tunnel to Streamlit\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"\\n\\nStreamlit app is available at: {public_url}\\n\\n\")\n",
        "\n",
        "# Run Streamlit app\n",
        "!streamlit run colab_streamlit_app.py &"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b47cce1",
      "metadata": {
        "id": "1b47cce1"
      },
      "source": [
        "## Step 10: Clean up (Run this when you're done)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ed325e96",
      "metadata": {
        "id": "ed325e96"
      },
      "outputs": [],
      "source": [
        "# Clean up\n",
        "!pkill -f streamlit  # Stop Streamlit if running\n",
        "ngrok.kill()  # Stop ngrok if running"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}