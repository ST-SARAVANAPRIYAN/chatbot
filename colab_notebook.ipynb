{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2619ae1d",
   "metadata": {},
   "source": [
    "# Running Chatbot with Google Gemini on Colab\n",
    "\n",
    "This notebook runs the chatbot project using Google's Gemini API on Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9541b812",
   "metadata": {},
   "source": [
    "## Step 1: Clone the GitHub repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae1e418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your GitHub repository\n",
    "!git clone https://github.com/ST-SARAVANAPRIYAN/chatbot.git\n",
    "%cd chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e2fe7c",
   "metadata": {},
   "source": [
    "## Step 2: Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e692606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install llama-index google-generativeai chromadb python-dotenv streamlit spacy transformers torch langchain\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2276b2e7",
   "metadata": {},
   "source": [
    "## Step 3: Set up your Gemini API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81956d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up your Gemini API key\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Securely input your API key\n",
    "GEMINI_API_KEY = getpass('Enter your Gemini API key: ')\n",
    "os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY\n",
    "\n",
    "# Create .env file with the API key\n",
    "with open('.env', 'w') as f:\n",
    "    f.write(f\"GEMINI_API_KEY={GEMINI_API_KEY}\\n\")\n",
    "    f.write(\"CHROMA_DB_DIRECTORY=./chroma_db\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5825b5b3",
   "metadata": {},
   "source": [
    "## Step 4: Create a version-flexible chatbot implementation\n",
    "\n",
    "This version handles different import structures in different versions of LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bdcec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile colab_chatbot_flexible.py\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import chromadb\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Directory paths\n",
    "DATA_DIR = \"./data\"\n",
    "CHROMA_DB_DIRECTORY = \"./chroma_db\"\n",
    "\n",
    "class GeminiChatbot:\n",
    "    def __init__(self):\n",
    "        # Get Gemini API key\n",
    "        self.api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            print(\"GEMINI_API_KEY not found in environment. Please set it first.\")\n",
    "            self.api_key = input(\"Enter your Gemini API key: \")\n",
    "            os.environ[\"GEMINI_API_KEY\"] = self.api_key\n",
    "        \n",
    "        # Configure Gemini\n",
    "        genai.configure(api_key=self.api_key)\n",
    "        self.model = genai.GenerativeModel('gemini-pro')\n",
    "        self.index = None\n",
    "        \n",
    "    def load_documents(self):\n",
    "        \"\"\"Load documents from the data directory\"\"\"\n",
    "        if not os.path.exists(DATA_DIR):\n",
    "            os.makedirs(DATA_DIR)\n",
    "            print(f\"Created data directory at {DATA_DIR}\")\n",
    "        \n",
    "        if not os.listdir(DATA_DIR):\n",
    "            print(\"Data directory is empty. Please add some documents.\")\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    def build_index(self):\n",
    "        \"\"\"Build vector index from documents\"\"\"\n",
    "        try:\n",
    "            # Dynamic imports to handle different versions of llama-index\n",
    "            try:\n",
    "                # Try importing from llama_index (newer versions)\n",
    "                from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
    "                from llama_index.node_parser import SentenceSplitter\n",
    "                from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "                print(\"Using llama_index package\")\n",
    "            except ImportError:\n",
    "                # Try importing from llama_index.core (older or different versions)\n",
    "                from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
    "                from llama_index.core.node_parser import SentenceSplitter\n",
    "                try:\n",
    "                    from llama_index.core.vector_stores.chroma import ChromaVectorStore\n",
    "                    print(\"Using llama_index.core package\")\n",
    "                except ImportError:\n",
    "                    from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "                    print(\"Using mixed llama_index imports\")\n",
    "            \n",
    "            # Load documents\n",
    "            documents = SimpleDirectoryReader(DATA_DIR).load_data()\n",
    "            \n",
    "            # Create sentence splitter for text chunking\n",
    "            text_splitter = SentenceSplitter(\n",
    "                chunk_size=1000,\n",
    "                chunk_overlap=200\n",
    "            )\n",
    "            \n",
    "            # Set up Chroma client\n",
    "            if not os.path.exists(CHROMA_DB_DIRECTORY):\n",
    "                os.makedirs(CHROMA_DB_DIRECTORY)\n",
    "                \n",
    "            chroma_client = chromadb.PersistentClient(path=CHROMA_DB_DIRECTORY)\n",
    "            chroma_collection = chroma_client.get_or_create_collection(\"documents\")\n",
    "            vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "            storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "            \n",
    "            # Build index\n",
    "            try:\n",
    "                self.index = VectorStoreIndex.from_documents(\n",
    "                    documents,\n",
    "                    storage_context=storage_context,\n",
    "                    transformations=[text_splitter],\n",
    "                )\n",
    "            except TypeError:\n",
    "                # Try alternative approach if transformations parameter doesn't work\n",
    "                self.index = VectorStoreIndex.from_documents(\n",
    "                    documents,\n",
    "                    storage_context=storage_context,\n",
    "                    node_parser=text_splitter,\n",
    "                )\n",
    "            \n",
    "            print(\"Index built successfully!\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error building index: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "            \n",
    "    def query(self, query_text):\n",
    "        \"\"\"Query the index with a natural language query\"\"\"\n",
    "        if not self.index:\n",
    "            print(\"Index not built yet. Please build the index first.\")\n",
    "            return {\n",
    "                \"answer\": \"Index not built yet. Please build the index first.\",\n",
    "                \"sources\": []\n",
    "            }\n",
    "            \n",
    "        try:\n",
    "            # Create query engine\n",
    "            query_engine = self.index.as_query_engine()\n",
    "            \n",
    "            # Execute query\n",
    "            response = query_engine.query(query_text)\n",
    "            \n",
    "            # Extract source information\n",
    "            sources = []\n",
    "            if hasattr(response, 'source_nodes'):\n",
    "                for node in response.source_nodes:\n",
    "                    source = {\n",
    "                        \"text\": node.node.get_content(),\n",
    "                        \"metadata\": node.node.metadata,\n",
    "                        \"score\": node.score if hasattr(node, 'score') else None\n",
    "                    }\n",
    "                    sources.append(source)\n",
    "            \n",
    "            return {\n",
    "                \"answer\": str(response),\n",
    "                \"sources\": sources\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error during query: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return {\n",
    "                \"answer\": f\"Error during query: {str(e)}\",\n",
    "                \"sources\": []\n",
    "            }\n",
    "    \n",
    "    def chat_interface(self):\n",
    "        \"\"\"Simple chat interface for testing\"\"\"\n",
    "        print(\"\\n===== Gemini Chatbot =====\")\n",
    "        print(\"Type 'exit' to quit\")\n",
    "        print(\"========================\\n\")\n",
    "        \n",
    "        while True:\n",
    "            query = input(\"\\nYou: \")\n",
    "            if query.lower() in ['exit', 'quit', 'q']:\n",
    "                break\n",
    "                \n",
    "            response = self.query(query)\n",
    "            print(f\"\\nBot: {response['answer']}\")\n",
    "            \n",
    "            # Print sources\n",
    "            if response['sources']:\n",
    "                print(\"\\nSources:\")\n",
    "                for i, source in enumerate(response['sources'][:2], 1):\n",
    "                    source_name = source.get('metadata', {}).get('source', f\"Source {i}\")\n",
    "                    print(f\"- {source_name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Initializing chatbot...\")\n",
    "    chatbot = GeminiChatbot()\n",
    "    \n",
    "    print(\"Loading documents...\")\n",
    "    if chatbot.load_documents():\n",
    "        print(\"Building index...\")\n",
    "        if chatbot.build_index():\n",
    "            print(\"Ready to chat!\")\n",
    "            chatbot.chat_interface()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1520bf54",
   "metadata": {},
   "source": [
    "## Step 5: Upload custom content (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02b7662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your own content files (optional)\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "# Let user upload files\n",
    "print(\"Upload your content files (optional). Skip this if you want to use the sample FAQ.\")\n",
    "try:\n",
    "    uploaded = files.upload()\n",
    "\n",
    "    # Save uploaded files to the data directory\n",
    "    for filename, content in uploaded.items():\n",
    "        with open(f'data/{filename}', 'wb') as f:\n",
    "            f.write(content)\n",
    "        print(f\"Saved {filename} to data directory\")\n",
    "except Exception as e:\n",
    "    print(f\"No files uploaded or error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f465dc",
   "metadata": {},
   "source": [
    "## Step 6: Run the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a09518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the command-line chatbot\n",
    "!python colab_chatbot_flexible.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b217d15c",
   "metadata": {},
   "source": [
    "## Step 7: Create Streamlit Interface (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d010df8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile colab_streamlit_app.py\n",
    "import streamlit as st\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import chromadb\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Gemini\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if api_key:\n",
    "    genai.configure(api_key=api_key)\n",
    "\n",
    "# Directory paths\n",
    "DATA_DIR = \"./data\"\n",
    "CHROMA_DB_DIRECTORY = \"./chroma_db\"\n",
    "\n",
    "# Initialize session state\n",
    "if 'index' not in st.session_state:\n",
    "    st.session_state.index = None\n",
    "if 'messages' not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "def build_index():\n",
    "    \"\"\"Build or load the vector index\"\"\"\n",
    "    try:\n",
    "        # Dynamic imports to handle different versions of llama-index\n",
    "        try:\n",
    "            # Try importing from llama_index (newer versions)\n",
    "            from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
    "            from llama_index.node_parser import SentenceSplitter\n",
    "            from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "            st.info(\"Using llama_index package\")\n",
    "        except ImportError:\n",
    "            # Try importing from llama_index.core (older or different versions)\n",
    "            from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
    "            from llama_index.core.node_parser import SentenceSplitter\n",
    "            try:\n",
    "                from llama_index.core.vector_stores.chroma import ChromaVectorStore\n",
    "                st.info(\"Using llama_index.core package\")\n",
    "            except ImportError:\n",
    "                from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "                st.info(\"Using mixed llama_index imports\")\n",
    "\n",
    "        if not os.path.exists(DATA_DIR):\n",
    "            os.makedirs(DATA_DIR)\n",
    "            \n",
    "        # Load documents\n",
    "        documents = SimpleDirectoryReader(DATA_DIR).load_data()\n",
    "        \n",
    "        # Create sentence splitter for text chunking\n",
    "        text_splitter = SentenceSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        \n",
    "        # Set up Chroma client\n",
    "        if not os.path.exists(CHROMA_DB_DIRECTORY):\n",
    "            os.makedirs(CHROMA_DB_DIRECTORY)\n",
    "            \n",
    "        chroma_client = chromadb.PersistentClient(path=CHROMA_DB_DIRECTORY)\n",
    "        chroma_collection = chroma_client.get_or_create_collection(\"documents\")\n",
    "        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "        \n",
    "        # Build index\n",
    "        try:\n",
    "            index = VectorStoreIndex.from_documents(\n",
    "                documents,\n",
    "                storage_context=storage_context,\n",
    "                transformations=[text_splitter],\n",
    "            )\n",
    "        except TypeError:\n",
    "            # Try alternative approach if transformations parameter doesn't work\n",
    "            index = VectorStoreIndex.from_documents(\n",
    "                documents,\n",
    "                storage_context=storage_context,\n",
    "                node_parser=text_splitter,\n",
    "            )\n",
    "        \n",
    "        return index\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error building index: {str(e)}\")\n",
    "        import traceback\n",
    "        st.error(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "def query_index(query_text):\n",
    "    \"\"\"Query the index with a natural language query\"\"\"\n",
    "    if not st.session_state.index:\n",
    "        return {\n",
    "            \"answer\": \"Index not built yet. Please build the index first.\",\n",
    "            \"sources\": []\n",
    "        }\n",
    "        \n",
    "    try:\n",
    "        # Create query engine\n",
    "        query_engine = st.session_state.index.as_query_engine()\n",
    "        \n",
    "        # Execute query\n",
    "        response = query_engine.query(query_text)\n",
    "        \n",
    "        # Extract source information\n",
    "        sources = []\n",
    "        if hasattr(response, 'source_nodes'):\n",
    "            for node in response.source_nodes:\n",
    "                source = {\n",
    "                    \"text\": node.node.get_content(),\n",
    "                    \"metadata\": node.node.metadata,\n",
    "                    \"score\": node.score if hasattr(node, 'score') else None\n",
    "                }\n",
    "                sources.append(source)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": str(response),\n",
    "            \"sources\": sources\n",
    "        }\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        st.error(traceback.format_exc())\n",
    "        return {\n",
    "            \"answer\": f\"Error during query: {str(e)}\",\n",
    "            \"sources\": []\n",
    "        }\n",
    "\n",
    "# App title\n",
    "st.title(\"ðŸ¤– Gemini Chatbot\")\n",
    "\n",
    "# Sidebar\n",
    "with st.sidebar:\n",
    "    st.title(\"Settings\")\n",
    "    \n",
    "    # API key input\n",
    "    if not api_key:\n",
    "        new_api_key = st.text_input(\"Enter Gemini API Key\", type=\"password\")\n",
    "        if new_api_key:\n",
    "            genai.configure(api_key=new_api_key)\n",
    "            os.environ[\"GEMINI_API_KEY\"] = new_api_key\n",
    "            with open('.env', 'w') as f:\n",
    "                f.write(f\"GEMINI_API_KEY={new_api_key}\\n\")\n",
    "                f.write(\"CHROMA_DB_DIRECTORY=./chroma_db\\n\")\n",
    "            st.success(\"API key set!\")\n",
    "            api_key = new_api_key\n",
    "    \n",
    "    # Build index button\n",
    "    if st.button(\"Build/Rebuild Index\"):\n",
    "        with st.spinner(\"Building index...\"):\n",
    "            st.session_state.index = build_index()\n",
    "            if st.session_state.index:\n",
    "                st.success(\"Index built successfully!\")\n",
    "            else:\n",
    "                st.error(\"Failed to build index\")\n",
    "    \n",
    "    # Show data directory contents\n",
    "    st.subheader(\"Data Directory\")\n",
    "    if os.path.exists(DATA_DIR):\n",
    "        files = os.listdir(DATA_DIR)\n",
    "        if files:\n",
    "            st.write(f\"Found {len(files)} files:\")\n",
    "            for file in files:\n",
    "                st.write(f\"- {file}\")\n",
    "        else:\n",
    "            st.write(\"No files found in data directory\")\n",
    "    else:\n",
    "        st.write(\"Data directory does not exist\")\n",
    "\n",
    "    # Clear chat history\n",
    "    if st.button(\"Clear Chat History\"):\n",
    "        st.session_state.messages = []\n",
    "        st.success(\"Chat history cleared\")\n",
    "\n",
    "# Initialize or load index\n",
    "if st.session_state.index is None:\n",
    "    if os.path.exists(CHROMA_DB_DIRECTORY):\n",
    "        with st.spinner(\"Loading existing index...\"):\n",
    "            st.session_state.index = build_index()\n",
    "            if st.session_state.index:\n",
    "                st.success(\"Index loaded successfully!\")\n",
    "    else:\n",
    "        st.info(\"No index found. Please build the index using the sidebar button.\")\n",
    "\n",
    "# Display chat messages\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "        \n",
    "        # Show sources if available\n",
    "        if message[\"role\"] == \"assistant\" and \"sources\" in message:\n",
    "            with st.expander(\"View Sources\"):\n",
    "                for i, source in enumerate(message[\"sources\"], 1):\n",
    "                    source_name = source.get('metadata', {}).get('source', f\"Source {i}\")\n",
    "                    st.markdown(f\"**{source_name}**\")\n",
    "                    text = source.get('text', '')\n",
    "                    if text:\n",
    "                        st.text(text[:200] + \"...\" if len(text) > 200 else text)\n",
    "\n",
    "# Chat input\n",
    "if prompt := st.chat_input(\"Ask me anything...\"):\n",
    "    # Check if API key is set\n",
    "    if not api_key:\n",
    "        st.error(\"Please set your Gemini API key in the sidebar first.\")\n",
    "        st.stop()\n",
    "    \n",
    "    # Add user message to chat history\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    # Display user message\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "        \n",
    "    # Generate and display assistant response\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"Thinking...\"):\n",
    "            if st.session_state.index is None:\n",
    "                response_content = \"Please build the index first using the sidebar button.\"\n",
    "                st.markdown(response_content)\n",
    "                st.session_state.messages.append({\n",
    "                    \"role\": \"assistant\", \n",
    "                    \"content\": response_content\n",
    "                })\n",
    "            else:\n",
    "                try:\n",
    "                    response = query_index(prompt)\n",
    "                    st.markdown(response[\"answer\"])\n",
    "                    \n",
    "                    # Add assistant message to chat history\n",
    "                    st.session_state.messages.append({\n",
    "                        \"role\": \"assistant\", \n",
    "                        \"content\": response[\"answer\"],\n",
    "                        \"sources\": response[\"sources\"]\n",
    "                    })\n",
    "                    \n",
    "                    # Show sources if available\n",
    "                    if response[\"sources\"]:\n",
    "                        with st.expander(\"View Sources\"):\n",
    "                            for i, source in enumerate(response[\"sources\"], 1):\n",
    "                                source_name = source.get('metadata', {}).get('source', f\"Source {i}\")\n",
    "                                st.markdown(f\"**{source_name}**\")\n",
    "                                text = source.get('text', '')\n",
    "                                if text:\n",
    "                                    st.text(text[:200] + \"...\" if len(text) > 200 else text)\n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Error: {str(e)}\"\n",
    "                    st.error(error_msg)\n",
    "                    st.session_state.messages.append({\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": error_msg\n",
    "                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c61ea93",
   "metadata": {},
   "source": [
    "## Step 8: Create the Streamlit interface for Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037a342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile colab_streamlit_app.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Streamlit web interface for the chatbot - Google Colab version\n",
    "\"\"\"\n",
    "import streamlit as st\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import chromadb\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Directory paths\n",
    "DATA_DIR = \"./data\"\n",
    "CHROMA_DB_DIRECTORY = \"./chroma_db\"\n",
    "\n",
    "class GeminiChatbot:\n",
    "    def __init__(self):\n",
    "        # Get Gemini API key\n",
    "        self.api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            print(\"GEMINI_API_KEY not found in environment. Please set it first.\")\n",
    "            self.api_key = st.text_input(\"Enter your Gemini API key:\", type=\"password\")\n",
    "            if not self.api_key:\n",
    "                st.error(\"Gemini API key is required to continue.\")\n",
    "                st.stop()\n",
    "            os.environ[\"GEMINI_API_KEY\"] = self.api_key\n",
    "        \n",
    "        # Configure Gemini\n",
    "        genai.configure(api_key=self.api_key)\n",
    "        self.model = genai.GenerativeModel('gemini-pro')\n",
    "        self.index = None\n",
    "        \n",
    "    def load_documents(self):\n",
    "        \"\"\"Load documents from the data directory\"\"\"\n",
    "        if not os.path.exists(DATA_DIR):\n",
    "            os.makedirs(DATA_DIR)\n",
    "            st.info(f\"Created data directory at {DATA_DIR}\")\n",
    "        \n",
    "        if not os.listdir(DATA_DIR):\n",
    "            st.warning(\"Data directory is empty. Please add some documents.\")\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    def build_index(self):\n",
    "        \"\"\"Build vector index from documents\"\"\"\n",
    "        try:\n",
    "            # Dynamic imports to handle different versions of llama-index\n",
    "            try:\n",
    "                # Try importing from llama_index (newer versions)\n",
    "                from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
    "                from llama_index.node_parser import SentenceSplitter\n",
    "                from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "                st.info(\"Using llama_index package\")\n",
    "            except ImportError:\n",
    "                # Try importing from llama_index.core (older or different versions)\n",
    "                from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
    "                from llama_index.core.node_parser import SentenceSplitter\n",
    "                try:\n",
    "                    from llama_index.core.vector_stores.chroma import ChromaVectorStore\n",
    "                    st.info(\"Using llama_index.core package\")\n",
    "                except ImportError:\n",
    "                    from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "                    st.info(\"Using mixed llama_index imports\")\n",
    "            \n",
    "            # Load documents\n",
    "            documents = SimpleDirectoryReader(DATA_DIR).load_data()\n",
    "            \n",
    "            # Create sentence splitter for text chunking\n",
    "            text_splitter = SentenceSplitter(\n",
    "                chunk_size=1000,\n",
    "                chunk_overlap=200\n",
    "            )\n",
    "            \n",
    "            # Set up Chroma client\n",
    "            if not os.path.exists(CHROMA_DB_DIRECTORY):\n",
    "                os.makedirs(CHROMA_DB_DIRECTORY)\n",
    "                \n",
    "            chroma_client = chromadb.PersistentClient(path=CHROMA_DB_DIRECTORY)\n",
    "            chroma_collection = chroma_client.get_or_create_collection(\"documents\")\n",
    "            vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "            storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "            \n",
    "            # Build index with error handling for different parameter names\n",
    "            try:\n",
    "                self.index = VectorStoreIndex.from_documents(\n",
    "                    documents,\n",
    "                    storage_context=storage_context,\n",
    "                    transformations=[text_splitter],\n",
    "                )\n",
    "            except TypeError:\n",
    "                # Try alternative approach if transformations parameter doesn't work\n",
    "                self.index = VectorStoreIndex.from_documents(\n",
    "                    documents,\n",
    "                    storage_context=storage_context,\n",
    "                    node_parser=text_splitter,\n",
    "                )\n",
    "            \n",
    "            st.success(\"Index built successfully!\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error building index: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "            \n",
    "    def query(self, query_text):\n",
    "        \"\"\"Query the index with a natural language query\"\"\"\n",
    "        if not self.index:\n",
    "            st.error(\"Index not built yet. Please build the index first.\")\n",
    "            return {\n",
    "                \"answer\": \"Index not built yet. Please build the index first.\",\n",
    "                \"sources\": []\n",
    "            }\n",
    "            \n",
    "        try:\n",
    "            # Create query engine\n",
    "            query_engine = self.index.as_query_engine()\n",
    "            \n",
    "            # Execute query\n",
    "            response = query_engine.query(query_text)\n",
    "            \n",
    "            # Extract source information\n",
    "            sources = []\n",
    "            if hasattr(response, 'source_nodes'):\n",
    "                for node in response.source_nodes:\n",
    "                    source = {\n",
    "                        \"text\": node.node.get_content(),\n",
    "                        \"metadata\": node.node.metadata,\n",
    "                        \"score\": node.score if hasattr(node, 'score') else None\n",
    "                    }\n",
    "                    sources.append(source)\n",
    "            \n",
    "            return {\n",
    "                \"answer\": str(response),\n",
    "                \"sources\": sources\n",
    "            }\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error during query: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return {\n",
    "                \"answer\": f\"Error during query: {str(e)}\",\n",
    "                \"sources\": []\n",
    "            }\n",
    "\n",
    "def init_session_state():\n",
    "    \"\"\"Initialize session state variables\"\"\"\n",
    "    if 'messages' not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "    if 'chatbot' not in st.session_state:\n",
    "        st.session_state.chatbot = None\n",
    "\n",
    "def main():\n",
    "    # Set page config\n",
    "    st.set_page_config(\n",
    "        page_title=\"Gemini Chatbot for Google Colab\",\n",
    "        page_icon=\"ðŸ¤–\",\n",
    "        layout=\"wide\"\n",
    "    )\n",
    "    \n",
    "    # Initialize session state\n",
    "    init_session_state()\n",
    "    \n",
    "    # Main title\n",
    "    st.title(\"ðŸ¤– Gemini Chatbot for Google Colab\")\n",
    "    \n",
    "    # Sidebar\n",
    "    with st.sidebar:\n",
    "        st.title(\"Chatbot Settings\")\n",
    "        \n",
    "        # Initialize chatbot if not already done\n",
    "        if st.session_state.chatbot is None:\n",
    "            st.session_state.chatbot = GeminiChatbot()\n",
    "        \n",
    "        # Check data directory status\n",
    "        data_dir_status = \"âœ… Available\" if os.path.exists(DATA_DIR) and os.listdir(DATA_DIR) else \"âŒ Empty\"\n",
    "        st.info(f\"Data Directory: {data_dir_status}\")\n",
    "        \n",
    "        # Check index status\n",
    "        index_status = \"âœ… Available\" if os.path.exists(CHROMA_DB_DIRECTORY) else \"âŒ Not Built\"\n",
    "        st.info(f\"Vector Index: {index_status}\")\n",
    "        \n",
    "        # Upload documents\n",
    "        with st.expander(\"Upload Documents\"):\n",
    "            uploaded_file = st.file_uploader(\"Upload a document\", type=[\"txt\", \"md\", \"pdf\"])\n",
    "            if uploaded_file is not None:\n",
    "                if not os.path.exists(DATA_DIR):\n",
    "                    os.makedirs(DATA_DIR)\n",
    "                \n",
    "                file_path = os.path.join(DATA_DIR, uploaded_file.name)\n",
    "                with open(file_path, \"wb\") as f:\n",
    "                    f.write(uploaded_file.getvalue())\n",
    "                st.success(f\"File '{uploaded_file.name}' uploaded successfully\")\n",
    "        \n",
    "        # Build index button\n",
    "        if st.button(\"Build/Rebuild Vector Index\"):\n",
    "            if st.session_state.chatbot.load_documents():\n",
    "                with st.spinner(\"Building vector index...\"):\n",
    "                    if st.session_state.chatbot.build_index():\n",
    "                        st.success(\"Vector index built successfully!\")\n",
    "                    else:\n",
    "                        st.error(\"Failed to build vector index.\")\n",
    "            else:\n",
    "                st.error(\"No documents found. Please upload some documents first.\")\n",
    "    \n",
    "    # Display chat messages\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.markdown(message[\"content\"])\n",
    "            \n",
    "            # Show sources if available\n",
    "            if message[\"role\"] == \"assistant\" and \"sources\" in message and message[\"sources\"]:\n",
    "                with st.expander(\"View Sources\"):\n",
    "                    for i, source in enumerate(message[\"sources\"], 1):\n",
    "                        source_name = source.get('metadata', {}).get('source', f\"Source {i}\")\n",
    "                        st.markdown(f\"**Document: {source_name}**\")\n",
    "                        text = source.get('text', '')\n",
    "                        if text:\n",
    "                            st.text(text[:200] + \"...\" if len(text) > 200 else text)\n",
    "    \n",
    "    # Chat input\n",
    "    if prompt := st.chat_input(\"Ask me anything...\"):\n",
    "        # Add user message to chat history\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        # Display user message\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.markdown(prompt)\n",
    "        \n",
    "        # Check if index is built\n",
    "        if not st.session_state.chatbot.index:\n",
    "            if not os.path.exists(CHROMA_DB_DIRECTORY) or not os.listdir(CHROMA_DB_DIRECTORY):\n",
    "                # Need to build index first\n",
    "                if st.session_state.chatbot.load_documents():\n",
    "                    with st.spinner(\"Building index for the first time...\"):\n",
    "                        st.session_state.chatbot.build_index()\n",
    "                else:\n",
    "                    st.error(\"No documents found. Please upload some documents first.\")\n",
    "                    st.session_state.messages.append({\n",
    "                        \"role\": \"assistant\", \n",
    "                        \"content\": \"I need some documents to learn from. Please upload documents using the sidebar.\",\n",
    "                        \"sources\": []\n",
    "                    })\n",
    "                    st.stop()\n",
    "            else:\n",
    "                # Index exists but not loaded\n",
    "                with st.spinner(\"Loading existing index...\"):\n",
    "                    st.session_state.chatbot.build_index()\n",
    "        \n",
    "        # Generate and display assistant response\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            message_placeholder = st.empty()\n",
    "            with st.spinner(\"Thinking...\"):\n",
    "                response = st.session_state.chatbot.query(prompt)\n",
    "                message_placeholder.markdown(response[\"answer\"])\n",
    "                \n",
    "                # Add assistant message to chat history\n",
    "                st.session_state.messages.append({\n",
    "                    \"role\": \"assistant\", \n",
    "                    \"content\": response[\"answer\"],\n",
    "                    \"sources\": response[\"sources\"]\n",
    "                })\n",
    "                \n",
    "                # Show sources if available\n",
    "                if response[\"sources\"]:\n",
    "                    with st.expander(\"View Sources\"):\n",
    "                        for i, source in enumerate(response[\"sources\"], 1):\n",
    "                            source_name = source.get('metadata', {}).get('source', f\"Source {i}\")\n",
    "                            st.markdown(f\"**Document: {source_name}**\")\n",
    "                            text = source.get('text', '')\n",
    "                            if text:\n",
    "                                st.text(text[:200] + \"...\" if len(text) > 200 else text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf55f626",
   "metadata": {},
   "source": [
    "## Step 9: Run Streamlit Interface (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d5a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ngrok and pyngrok if you want to run the Streamlit app\n",
    "!pip install pyngrok\n",
    "\n",
    "# Run Streamlit app using ngrok\n",
    "from pyngrok import ngrok\n",
    "import os\n",
    "\n",
    "# Set up ngrok authentication (optional, needed for better reliability)\n",
    "# Get your token from https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "NGROK_AUTH_TOKEN = input(\"Enter your ngrok auth token (optional, press Enter to skip): \")\n",
    "if NGROK_AUTH_TOKEN:\n",
    "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "\n",
    "# Start ngrok tunnel to Streamlit\n",
    "public_url = ngrok.connect(8501)\n",
    "print(f\"\\n\\nStreamlit app is available at: {public_url}\\n\\n\")\n",
    "\n",
    "# Run Streamlit app\n",
    "!streamlit run colab_streamlit_app.py &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b47cce1",
   "metadata": {},
   "source": [
    "## Step 10: Clean up (Run this when you're done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed325e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "!pkill -f streamlit  # Stop Streamlit if running\n",
    "ngrok.kill()  # Stop ngrok if running"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
