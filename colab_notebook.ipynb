{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2619ae1d",
      "metadata": {
        "id": "2619ae1d"
      },
      "source": [
        "# Running Chatbot with Google Gemini on Colab\n",
        "\n",
        "This notebook runs the chatbot project using Google's Gemini API on Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9541b812",
      "metadata": {
        "id": "9541b812"
      },
      "source": [
        "## Step 1: Clone the GitHub repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "dae1e418",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dae1e418",
        "outputId": "44d38050-320a-45ff-9838-268f901530c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'chatbot'...\n",
            "remote: Enumerating objects: 45, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 45 (delta 11), reused 38 (delta 7), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (45/45), 60.76 KiB | 10.13 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n",
            "/content/chatbot\n"
          ]
        }
      ],
      "source": [
        "# Clone your GitHub repository\n",
        "!git clone https://github.com/ST-SARAVANAPRIYAN/chatbot.git\n",
        "%cd chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9e2fe7c",
      "metadata": {
        "id": "c9e2fe7c"
      },
      "source": [
        "## Step 2: Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3e692606",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e692606",
        "outputId": "6873510c-6405-425c-b61e-0c3d50bd957f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index==0.12.33\n",
            "  Downloading llama_index-0.12.33-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting llama-index-core\n",
            "  Downloading llama_index_core-0.12.46-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-embeddings-gemini==0.1.0\n",
            "  Downloading llama_index_embeddings_gemini-0.1.0-py3-none-any.whl.metadata (646 bytes)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.46.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index==0.12.33)\n",
            "  Downloading llama_index_agent_openai-0.4.12-py3-none-any.whl.metadata (439 bytes)\n",
            "Collecting llama-index-cli<0.5.0,>=0.4.1 (from llama-index==0.12.33)\n",
            "  Downloading llama_index_cli-0.4.3-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index==0.12.33)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index==0.12.33)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.7.8-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index==0.12.33)\n",
            "  Downloading llama_index_llms_openai-0.3.44-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index==0.12.33)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index==0.12.33)\n",
            "  Downloading llama_index_program_openai-0.3.2-py3-none-any.whl.metadata (473 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index==0.12.33)\n",
            "  Downloading llama_index_question_gen_openai-0.3.1-py3-none-any.whl.metadata (492 bytes)\n",
            "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index==0.12.33)\n",
            "  Downloading llama_index_readers_file-0.4.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index==0.12.33)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.12.33) (3.9.1)\n",
            "Collecting google-generativeai<0.4.0,>=0.3.2 (from llama-index-embeddings-gemini==0.1.0)\n",
            "  Downloading google_generativeai-0.3.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "INFO: pip is looking at multiple versions of llama-index-embeddings-gemini to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Cannot install llama-index-core, llama-index-embeddings-gemini==0.1.0 and llama-index==0.12.33 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    The user requested llama-index-core\n",
            "    llama-index 0.12.33 depends on llama-index-core<0.13.0 and >=0.12.33\n",
            "    llama-index-embeddings-gemini 0.1.0 depends on llama-index-core==0.10.0\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m132.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install llama-index==0.12.33 llama-index-core llama-index-embeddings-gemini==0.1.0 chromadb python-dotenv streamlit spacy transformers torch langchain\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index[embeddings-openai,vector-stores-chroma,llms-openai]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qPYs0zZCRVQh",
        "outputId": "0b94f4f9-3648-488c-ad55-14fc53912542"
      },
      "id": "qPYs0zZCRVQh",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index[embeddings-openai,llms-openai,vector-stores-chroma]\n",
            "  Downloading llama_index-0.12.46-py3-none-any.whl.metadata (12 kB)\n",
            "\u001b[33mWARNING: llama-index 0.12.46 does not provide the extra 'embeddings-openai'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: llama-index 0.12.46 does not provide the extra 'llms-openai'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: llama-index 0.12.46 does not provide the extra 'vector-stores-chroma'\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting llama-index-agent-openai<0.5,>=0.4.0 (from llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Using cached llama_index_agent_openai-0.4.12-py3-none-any.whl.metadata (439 bytes)\n",
            "Collecting llama-index-cli<0.5,>=0.4.2 (from llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Using cached llama_index_cli-0.4.3-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting llama-index-core<0.13,>=0.12.46 (from llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Using cached llama_index_core-0.12.46-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-embeddings-openai<0.4,>=0.3.0 (from llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Using cached llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Using cached llama_index_indices_managed_llama_cloud-0.7.8-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-llms-openai<0.5,>=0.4.0 (from llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Downloading llama_index_llms_openai-0.4.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.6,>=0.5.0 (from llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.5.1-py3-none-any.whl.metadata (440 bytes)\n",
            "Collecting llama-index-program-openai<0.4,>=0.3.0 (from llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Using cached llama_index_program_openai-0.3.2-py3-none-any.whl.metadata (473 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4,>=0.3.0 (from llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Using cached llama_index_question_gen_openai-0.3.1-py3-none-any.whl.metadata (492 bytes)\n",
            "Collecting llama-index-readers-file<0.5,>=0.4.0 (from llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Using cached llama_index_readers_file-0.4.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Using cached llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (3.9.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5,>=0.4.0->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (1.93.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (3.11.15)\n",
            "Collecting aiosqlite (from llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting banks<3,>=2.0.0 (from llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Downloading banks-2.1.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dataclasses-json (from llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2,>=1.2.0 (from llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (2025.3.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (0.28.1)\n",
            "Collecting llama-index-workflows<2,>=1.0.1 (from llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Downloading llama_index_workflows-1.0.1-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (11.2.1)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (2.11.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (2.32.3)\n",
            "Collecting setuptools>=80.9.0 (from llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (2.0.41)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (4.14.0)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (1.17.2)\n",
            "Collecting llama-cloud==0.1.30 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Downloading llama_cloud-0.1.30-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud==0.1.30->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (2025.6.15)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (4.13.4)\n",
            "Requirement already satisfied: pandas<2.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (2.2.2)\n",
            "Collecting pypdf<6,>=5.1.0 (from llama-index-readers-file<0.5,>=0.4.0->llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Downloading pypdf-5.7.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5,>=0.4.0->llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Downloading llama_parse-0.6.41-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (1.20.1)\n",
            "Collecting griffe (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Downloading griffe-1.7.3-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (3.1.6)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (4.3.8)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.5,>=0.4.0->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (2.7)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (0.16.0)\n",
            "Collecting llama-index-instrumentation>=0.1.0 (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Downloading llama_index_instrumentation-0.2.0-py3-none-any.whl.metadata (252 bytes)\n",
            "Collecting llama-cloud-services>=0.6.41 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Downloading llama_cloud_services-0.6.41-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (3.2.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.1 (from llama-cloud-services>=0.6.41->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Using cached python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (1.17.0)\n",
            "Collecting colorama>=0.4 (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma])\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.46->llama-index[embeddings-openai,llms-openai,vector-stores-chroma]) (3.0.2)\n",
            "Downloading llama_index_agent_openai-0.4.12-py3-none-any.whl (14 kB)\n",
            "Downloading llama_index_cli-0.4.3-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_core-0.12.46-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.7.8-py3-none-any.whl (16 kB)\n",
            "Downloading llama_cloud-0.1.30-py3-none-any.whl (282 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.1/282.1 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.4.7-py3-none-any.whl (25 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.5.1-py3-none-any.whl (3.4 kB)\n",
            "Downloading llama_index_program_openai-0.3.2-py3-none-any.whl (6.1 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.1-py3-none-any.whl (3.7 kB)\n",
            "Downloading llama_index_readers_file-0.4.9-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading llama_index-0.12.46-py3-none-any.whl (7.1 kB)\n",
            "Downloading banks-2.1.3-py3-none-any.whl (28 kB)\n",
            "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_index_workflows-1.0.1-py3-none-any.whl (36 kB)\n",
            "Downloading llama_parse-0.6.41-py3-none-any.whl (4.9 kB)\n",
            "Downloading pypdf-5.7.0-py3-none-any.whl (305 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.5/305.5 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading llama_cloud_services-0.6.41-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_instrumentation-0.2.0-py3-none-any.whl (14 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading griffe-1.7.3-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: striprtf, filetype, dirtyjson, setuptools, python-dotenv, pypdf, mypy-extensions, marshmallow, deprecated, colorama, aiosqlite, typing-inspect, griffe, llama-index-instrumentation, llama-cloud, dataclasses-json, banks, llama-index-workflows, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiosqlite-0.21.0 banks-2.1.3 colorama-0.4.6 dataclasses-json-0.6.7 deprecated-1.2.18 dirtyjson-1.0.8 filetype-1.2.0 griffe-1.7.3 llama-cloud-0.1.30 llama-cloud-services-0.6.41 llama-index-0.12.46 llama-index-agent-openai-0.4.12 llama-index-cli-0.4.3 llama-index-core-0.12.46 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.7.8 llama-index-instrumentation-0.2.0 llama-index-llms-openai-0.4.7 llama-index-multi-modal-llms-openai-0.5.1 llama-index-program-openai-0.3.2 llama-index-question-gen-openai-0.3.1 llama-index-readers-file-0.4.9 llama-index-readers-llama-parse-0.4.0 llama-index-workflows-1.0.1 llama-parse-0.6.41 marshmallow-3.26.1 mypy-extensions-1.1.0 pypdf-5.7.0 python-dotenv-1.1.1 setuptools-80.9.0 striprtf-0.0.26 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources"
                ]
              },
              "id": "6da229a024864db1b68c71a9b06f3963"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip show llama-index llama-index-core llama-index-embeddings-google-genai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7qz49C0Rf0N",
        "outputId": "7b4c2bf1-0daf-4c79-cc2b-6d8b47625b71"
      },
      "id": "m7qz49C0Rf0N",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: llama-index\n",
            "Version: 0.12.46\n",
            "Summary: Interface between LLMs and your data\n",
            "Home-page: https://llamaindex.ai\n",
            "Author: \n",
            "Author-email: Jerry Liu <jerry@llamaindex.ai>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: llama-index-agent-openai, llama-index-cli, llama-index-core, llama-index-embeddings-openai, llama-index-indices-managed-llama-cloud, llama-index-llms-openai, llama-index-multi-modal-llms-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index-readers-file, llama-index-readers-llama-parse, nltk\n",
            "Required-by: \n",
            "---\n",
            "Name: llama-index-core\n",
            "Version: 0.12.46\n",
            "Summary: Interface between LLMs and your data\n",
            "Home-page: https://llamaindex.ai\n",
            "Author: \n",
            "Author-email: Jerry Liu <jerry@llamaindex.ai>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: aiohttp, aiosqlite, banks, dataclasses-json, deprecated, dirtyjson, filetype, fsspec, httpx, llama-index-workflows, nest-asyncio, networkx, nltk, numpy, pillow, pydantic, pyyaml, requests, setuptools, sqlalchemy, tenacity, tiktoken, tqdm, typing-extensions, typing-inspect, wrapt\n",
            "Required-by: llama-cloud-services, llama-index, llama-index-agent-openai, llama-index-cli, llama-index-embeddings-google-genai, llama-index-embeddings-openai, llama-index-indices-managed-llama-cloud, llama-index-llms-openai, llama-index-multi-modal-llms-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index-readers-file, llama-index-readers-llama-parse\n",
            "---\n",
            "Name: llama-index-embeddings-google-genai\n",
            "Version: 0.2.1\n",
            "Summary: llama-index embeddings google genai integration\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: Your Name <you@example.com>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: google-genai, llama-index-core\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2276b2e7",
      "metadata": {
        "id": "2276b2e7"
      },
      "source": [
        "## Step 3: Set up your Gemini API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "81956d90",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81956d90",
        "outputId": "abee12ef-d511-4853-e943-fd81ca015f90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Gemini API key: ··········\n"
          ]
        }
      ],
      "source": [
        "# Set up your Gemini API key\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Securely input your API key\n",
        "GEMINI_API_KEY = getpass('Enter your Gemini API key: ')\n",
        "os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY\n",
        "\n",
        "# Create .env file with the API key\n",
        "with open('.env', 'w') as f:\n",
        "    f.write(f\"GEMINI_API_KEY={GEMINI_API_KEY}\\n\")\n",
        "    f.write(\"CHROMA_DB_DIRECTORY=./chroma_db\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5825b5b3",
      "metadata": {
        "id": "5825b5b3"
      },
      "source": [
        "## Step 4: Create a version-flexible chatbot implementation\n",
        "\n",
        "This version handles different import structures in different versions of LlamaIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f6bdcec8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6bdcec8",
        "outputId": "e810bb42-5c79-453f-a06a-ea0e61a116d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting colab_chatbot_flexible.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile colab_chatbot_flexible.py\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "import google.generativeai as genai\n",
        "from dotenv import load_dotenv\n",
        "import chromadb\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Directory paths\n",
        "DATA_DIR = \"./data\"\n",
        "CHROMA_DB_DIRECTORY = \"./chroma_db\"\n",
        "\n",
        "class GeminiChatbot:\n",
        "    def __init__(self):\n",
        "        # Get Gemini API key\n",
        "        self.api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            print(\"GEMINI_API_KEY not found in environment. Please set it first.\")\n",
        "            self.api_key = input(\"Enter your Gemini API key: \")\n",
        "            os.environ[\"GEMINI_API_KEY\"] = self.api_key\n",
        "\n",
        "        # Configure Gemini\n",
        "        genai.configure(api_key=self.api_key)\n",
        "        self.model = genai.GenerativeModel('gemini-pro')\n",
        "        self.index = None\n",
        "\n",
        "    def load_documents(self):\n",
        "        \"\"\"Load documents from the data directory\"\"\"\n",
        "        if not os.path.exists(DATA_DIR):\n",
        "            os.makedirs(DATA_DIR)\n",
        "            print(f\"Created data directory at {DATA_DIR}\")\n",
        "\n",
        "        if not os.listdir(DATA_DIR):\n",
        "            print(\"Data directory is empty. Please add some documents.\")\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def build_index(self):\n",
        "        \"\"\"Build vector index from documents\"\"\"\n",
        "        try:\n",
        "            # Dynamic imports to handle different versions of llama-index\n",
        "            try:\n",
        "                # Try importing from llama_index (newer versions)\n",
        "                from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
        "                from llama_index.node_parser import SentenceSplitter\n",
        "                from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "                print(\"Using llama_index package\")\n",
        "            except ImportError:\n",
        "                # Try importing from llama_index.core (older or different versions)\n",
        "                from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
        "                from llama_index.core.node_parser import SentenceSplitter\n",
        "                try:\n",
        "                    from llama_index.core.vector_stores.chroma import ChromaVectorStore\n",
        "                    print(\"Using llama_index.core package\")\n",
        "                except ImportError:\n",
        "                    from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "                    print(\"Using mixed llama_index imports\")\n",
        "\n",
        "            # Load documents\n",
        "            documents = SimpleDirectoryReader(DATA_DIR).load_data()\n",
        "\n",
        "            # Create sentence splitter for text chunking\n",
        "            text_splitter = SentenceSplitter(\n",
        "                chunk_size=1000,\n",
        "                chunk_overlap=200\n",
        "            )\n",
        "\n",
        "            # Set up Chroma client\n",
        "            if not os.path.exists(CHROMA_DB_DIRECTORY):\n",
        "                os.makedirs(CHROMA_DB_DIRECTORY)\n",
        "\n",
        "            chroma_client = chromadb.PersistentClient(path=CHROMA_DB_DIRECTORY)\n",
        "            chroma_collection = chroma_client.get_or_create_collection(\"documents\")\n",
        "            vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "            storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "            # Build index\n",
        "            try:\n",
        "                self.index = VectorStoreIndex.from_documents(\n",
        "                    documents,\n",
        "                    storage_context=storage_context,\n",
        "                    transformations=[text_splitter],\n",
        "                )\n",
        "            except TypeError:\n",
        "                # Try alternative approach if transformations parameter doesn't work\n",
        "                self.index = VectorStoreIndex.from_documents(\n",
        "                    documents,\n",
        "                    storage_context=storage_context,\n",
        "                    node_parser=text_splitter,\n",
        "                )\n",
        "\n",
        "            print(\"Index built successfully!\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error building index: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return False\n",
        "\n",
        "    def query(self, query_text):\n",
        "        \"\"\"Query the index with a natural language query\"\"\"\n",
        "        if not self.index:\n",
        "            print(\"Index not built yet. Please build the index first.\")\n",
        "            return {\n",
        "                \"answer\": \"Index not built yet. Please build the index first.\",\n",
        "                \"sources\": []\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            # Create query engine\n",
        "            query_engine = self.index.as_query_engine()\n",
        "\n",
        "            # Execute query\n",
        "            response = query_engine.query(query_text)\n",
        "\n",
        "            # Extract source information\n",
        "            sources = []\n",
        "            if hasattr(response, 'source_nodes'):\n",
        "                for node in response.source_nodes:\n",
        "                    source = {\n",
        "                        \"text\": node.node.get_content(),\n",
        "                        \"metadata\": node.node.metadata,\n",
        "                        \"score\": node.score if hasattr(node, 'score') else None\n",
        "                    }\n",
        "                    sources.append(source)\n",
        "\n",
        "            return {\n",
        "                \"answer\": str(response),\n",
        "                \"sources\": sources\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error during query: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return {\n",
        "                \"answer\": f\"Error during query: {str(e)}\",\n",
        "                \"sources\": []\n",
        "            }\n",
        "\n",
        "    def chat_interface(self):\n",
        "        \"\"\"Simple chat interface for testing\"\"\"\n",
        "        print(\"\\n===== Gemini Chatbot =====\")\n",
        "        print(\"Type 'exit' to quit\")\n",
        "        print(\"========================\\n\")\n",
        "\n",
        "        while True:\n",
        "            query = input(\"\\nYou: \")\n",
        "            if query.lower() in ['exit', 'quit', 'q']:\n",
        "                break\n",
        "\n",
        "            response = self.query(query)\n",
        "            print(f\"\\nBot: {response['answer']}\")\n",
        "\n",
        "            # Print sources\n",
        "            if response['sources']:\n",
        "                print(\"\\nSources:\")\n",
        "                for i, source in enumerate(response['sources'][:2], 1):\n",
        "                    source_name = source.get('metadata', {}).get('source', f\"Source {i}\")\n",
        "                    print(f\"- {source_name}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Initializing chatbot...\")\n",
        "    chatbot = GeminiChatbot()\n",
        "\n",
        "    print(\"Loading documents...\")\n",
        "    if chatbot.load_documents():\n",
        "        print(\"Building index...\")\n",
        "        if chatbot.build_index():\n",
        "            print(\"Ready to chat!\")\n",
        "            chatbot.chat_interface()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1520bf54",
      "metadata": {
        "id": "1520bf54"
      },
      "source": [
        "## Step 5: Upload custom content (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f02b7662",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "f02b7662",
        "outputId": "bbb64dc1-93f1-4ffb-f245-ecdfbd9de52d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your content files (optional). Skip this if you want to use the sample FAQ.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-50b723ed-3599-43aa-ae27-6d3f989be354\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-50b723ed-3599-43aa-ae27-6d3f989be354\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Upload your own content files (optional)\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Create data directory if it doesn't exist\n",
        "if not os.path.exists('data'):\n",
        "    os.makedirs('data')\n",
        "\n",
        "# Let user upload files\n",
        "print(\"Upload your content files (optional). Skip this if you want to use the sample FAQ.\")\n",
        "try:\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Save uploaded files to the data directory\n",
        "    for filename, content in uploaded.items():\n",
        "        with open(f'data/{filename}', 'wb') as f:\n",
        "            f.write(content)\n",
        "        print(f\"Saved {filename} to data directory\")\n",
        "except Exception as e:\n",
        "    print(f\"No files uploaded or error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15f465dc",
      "metadata": {
        "id": "15f465dc"
      },
      "source": [
        "## Step 6: Run the chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "12a09518",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12a09518",
        "outputId": "d4dabcf7-2858-417c-b91b-acd19762bed8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing chatbot...\n",
            "Loading documents...\n",
            "Building index...\n",
            "Using mixed llama_index imports\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
            "Error building index: \n",
            "******\n",
            "Could not load OpenAI embedding model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\n",
            "Original error:\n",
            "No API key found for OpenAI.\n",
            "Please set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\n",
            "API keys can be found or created at https://platform.openai.com/account/api-keys\n",
            "\n",
            "Consider using embed_model='local'.\n",
            "Visit our documentation for more embedding options: https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#modules\n",
            "******\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/llama_index/core/embeddings/utils.py\", line 59, in resolve_embed_model\n",
            "    validate_openai_api_key(embed_model.api_key)  # type: ignore\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/llama_index/embeddings/openai/utils.py\", line 103, in validate_openai_api_key\n",
            "    raise ValueError(MISSING_API_KEY_ERROR_MESSAGE)\n",
            "ValueError: No API key found for OpenAI.\n",
            "Please set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\n",
            "API keys can be found or created at https://platform.openai.com/account/api-keys\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/chatbot/colab_chatbot_flexible.py\", line 86, in build_index\n",
            "    self.index = VectorStoreIndex.from_documents(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/llama_index/core/indices/base.py\", line 122, in from_documents\n",
            "    return cls(\n",
            "           ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/llama_index/core/indices/vector_store/base.py\", line 73, in __init__\n",
            "    else Settings.embed_model\n",
            "         ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/llama_index/core/settings.py\", line 64, in embed_model\n",
            "    self._embed_model = resolve_embed_model(\"default\")\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/llama_index/core/embeddings/utils.py\", line 66, in resolve_embed_model\n",
            "    raise ValueError(\n",
            "ValueError: \n",
            "******\n",
            "Could not load OpenAI embedding model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\n",
            "Original error:\n",
            "No API key found for OpenAI.\n",
            "Please set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\n",
            "API keys can be found or created at https://platform.openai.com/account/api-keys\n",
            "\n",
            "Consider using embed_model='local'.\n",
            "Visit our documentation for more embedding options: https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#modules\n",
            "******\n"
          ]
        }
      ],
      "source": [
        "# Run the command-line chatbot\n",
        "!python colab_chatbot_flexible.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umjRGEXWSPhV",
        "outputId": "d399e315-273c-437e-9b00-a5af3204f75e"
      },
      "id": "umjRGEXWSPhV",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Using cached chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.7)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.14.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.73.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.18)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.24.0)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.26.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.33.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.5)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.34.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=ac925934679dabc57e5ca48b0379b108cef8ec5aa62c85203dc4b26d0ce978fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, pybase64, overrides, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-semantic-conventions, onnxruntime, kubernetes, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "Successfully installed backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.15 coloredlogs-15.0.1 durationpy-0.10 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 mmh3-5.1.0 onnxruntime-1.22.0 opentelemetry-api-1.34.1 opentelemetry-exporter-otlp-proto-common-1.34.1 opentelemetry-exporter-otlp-proto-grpc-1.34.1 opentelemetry-proto-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 overrides-7.7.0 posthog-5.4.0 pybase64-1.4.1 pypika-0.48.9 uvloop-0.21.0 watchfiles-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install llama-index-vector-stores-chroma\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LdOYmW8ShhH",
        "outputId": "a34f0160-13bc-4fb6-966e-081fc052a64b"
      },
      "id": "7LdOYmW8ShhH",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-vector-stores-chroma\n",
            "  Downloading llama_index_vector_stores_chroma-0.4.2-py3-none-any.whl.metadata (413 bytes)\n",
            "Requirement already satisfied: chromadb>=0.5.17 in /usr/local/lib/python3.11/dist-packages (from llama-index-vector-stores-chroma) (1.0.15)\n",
            "Requirement already satisfied: llama-index-core<0.13,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-vector-stores-chroma) (0.12.46)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.11.7)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.4.1)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.0.2)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (4.14.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.34.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.21.2)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.73.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.16.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (33.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.10.18)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (4.24.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (3.11.15)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (2.1.3)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (2025.3.2)\n",
            "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (1.0.1)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (3.5)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (3.9.1)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (11.2.1)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (2.0.41)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (0.9.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (1.17.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (1.20.1)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (3.1.6)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (4.3.8)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.16.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.26.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.4.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.10)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (0.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (2024.11.6)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.5.17->llama-index-vector-stores-chroma) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.5.17->llama-index-vector-stores-chroma) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.5.17->llama-index-vector-stores-chroma) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.55b1)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (3.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (3.2.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.33.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.5.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (1.1.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.1.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (15.0.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (3.26.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.18.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.1.5)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=0.5.17->llama-index-vector-stores-chroma) (10.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-vector-stores-chroma) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.6.1)\n",
            "Downloading llama_index_vector_stores_chroma-0.4.2-py3-none-any.whl (6.0 kB)\n",
            "Installing collected packages: llama-index-vector-stores-chroma\n",
            "Successfully installed llama-index-vector-stores-chroma-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample document in the data directory\n",
        "!mkdir -p data\n",
        "%%writefile data/sample_faq.md\n",
        "# Sample FAQ\n",
        "\n",
        "## What is this chatbot?\n",
        "This is a RAG-based chatbot using Google Gemini API.\n",
        "\n",
        "## What is RAG?\n",
        "RAG stands for Retrieval-Augmented Generation, which enhances LLM responses with retrieved information.\n",
        "\n",
        "## How does it work?\n",
        "It uses vector embeddings to find relevant information and then generates responses based on that information."
      ],
      "metadata": {
        "id": "iCKhYdNGMT6B"
      },
      "id": "iCKhYdNGMT6B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the chatbot in the same Colab notebook\n",
        "%run colab_chatbot_flexible.py"
      ],
      "metadata": {
        "id": "Cgpw0Ep7MtCs"
      },
      "id": "Cgpw0Ep7MtCs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PvlxiDFfPusn"
      },
      "id": "PvlxiDFfPusn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Try to get the API key from Colab secrets\n",
        "try:\n",
        "  GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "except:\n",
        "  # If not in secrets, ask user for it\n",
        "  GEMINI_API_KEY = input('Enter your Gemini API key: ')\n",
        "\n",
        "os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY\n",
        "\n",
        "# Create .env file with the API key\n",
        "with open('.env', 'w') as f:\n",
        "    f.write(f\"GEMINI_API_KEY={GEMINI_API_KEY}\\n\")\n",
        "\n",
        "print(\"API key set successfully!\")"
      ],
      "metadata": {
        "id": "NS_o-mQTMgCS"
      },
      "id": "NS_o-mQTMgCS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b217d15c",
      "metadata": {
        "id": "b217d15c"
      },
      "source": [
        "## Step 7: Create Streamlit Interface (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d010df8f",
      "metadata": {
        "id": "d010df8f"
      },
      "outputs": [],
      "source": [
        "%%writefile colab_streamlit_app.py\n",
        "import streamlit as st\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "from dotenv import load_dotenv\n",
        "import chromadb\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Configure Gemini\n",
        "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "if api_key:\n",
        "    genai.configure(api_key=api_key)\n",
        "\n",
        "# Directory paths\n",
        "DATA_DIR = \"./data\"\n",
        "CHROMA_DB_DIRECTORY = \"./chroma_db\"\n",
        "\n",
        "# Initialize session state\n",
        "if 'index' not in st.session_state:\n",
        "    st.session_state.index = None\n",
        "if 'messages' not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "def build_index():\n",
        "    \"\"\"Build or load the vector index\"\"\"\n",
        "    try:\n",
        "        # Dynamic imports to handle different versions of llama-index\n",
        "        try:\n",
        "            # Try importing from llama_index (newer versions)\n",
        "            from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
        "            from llama_index.node_parser import SentenceSplitter\n",
        "            from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "            st.info(\"Using llama_index package\")\n",
        "        except ImportError:\n",
        "            # Try importing from llama_index.core (older or different versions)\n",
        "            from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
        "            from llama_index.core.node_parser import SentenceSplitter\n",
        "            try:\n",
        "                from llama_index.core.vector_stores.chroma import ChromaVectorStore\n",
        "                st.info(\"Using llama_index.core package\")\n",
        "            except ImportError:\n",
        "                from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "                st.info(\"Using mixed llama_index imports\")\n",
        "\n",
        "        if not os.path.exists(DATA_DIR):\n",
        "            os.makedirs(DATA_DIR)\n",
        "\n",
        "        # Load documents\n",
        "        documents = SimpleDirectoryReader(DATA_DIR).load_data()\n",
        "\n",
        "        # Create sentence splitter for text chunking\n",
        "        text_splitter = SentenceSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200\n",
        "        )\n",
        "\n",
        "        # Set up Chroma client\n",
        "        if not os.path.exists(CHROMA_DB_DIRECTORY):\n",
        "            os.makedirs(CHROMA_DB_DIRECTORY)\n",
        "\n",
        "        chroma_client = chromadb.PersistentClient(path=CHROMA_DB_DIRECTORY)\n",
        "        chroma_collection = chroma_client.get_or_create_collection(\"documents\")\n",
        "        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "        # Build index\n",
        "        try:\n",
        "            index = VectorStoreIndex.from_documents(\n",
        "                documents,\n",
        "                storage_context=storage_context,\n",
        "                transformations=[text_splitter],\n",
        "            )\n",
        "        except TypeError:\n",
        "            # Try alternative approach if transformations parameter doesn't work\n",
        "            index = VectorStoreIndex.from_documents(\n",
        "                documents,\n",
        "                storage_context=storage_context,\n",
        "                node_parser=text_splitter,\n",
        "            )\n",
        "\n",
        "        return index\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error building index: {str(e)}\")\n",
        "        import traceback\n",
        "        st.error(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "def query_index(query_text):\n",
        "    \"\"\"Query the index with a natural language query\"\"\"\n",
        "    if not st.session_state.index:\n",
        "        return {\n",
        "            \"answer\": \"Index not built yet. Please build the index first.\",\n",
        "            \"sources\": []\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        # Create query engine\n",
        "        query_engine = st.session_state.index.as_query_engine()\n",
        "\n",
        "        # Execute query\n",
        "        response = query_engine.query(query_text)\n",
        "\n",
        "        # Extract source information\n",
        "        sources = []\n",
        "        if hasattr(response, 'source_nodes'):\n",
        "            for node in response.source_nodes:\n",
        "                source = {\n",
        "                    \"text\": node.node.get_content(),\n",
        "                    \"metadata\": node.node.metadata,\n",
        "                    \"score\": node.score if hasattr(node, 'score') else None\n",
        "                }\n",
        "                sources.append(source)\n",
        "\n",
        "        return {\n",
        "            \"answer\": str(response),\n",
        "            \"sources\": sources\n",
        "        }\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        st.error(traceback.format_exc())\n",
        "        return {\n",
        "            \"answer\": f\"Error during query: {str(e)}\",\n",
        "            \"sources\": []\n",
        "        }\n",
        "\n",
        "# App title\n",
        "st.title(\"🤖 Gemini Chatbot\")\n",
        "\n",
        "# Sidebar\n",
        "with st.sidebar:\n",
        "    st.title(\"Settings\")\n",
        "\n",
        "    # API key input\n",
        "    if not api_key:\n",
        "        new_api_key = st.text_input(\"Enter Gemini API Key\", type=\"password\")\n",
        "        if new_api_key:\n",
        "            genai.configure(api_key=new_api_key)\n",
        "            os.environ[\"GEMINI_API_KEY\"] = new_api_key\n",
        "            with open('.env', 'w') as f:\n",
        "                f.write(f\"GEMINI_API_KEY={new_api_key}\\n\")\n",
        "                f.write(\"CHROMA_DB_DIRECTORY=./chroma_db\\n\")\n",
        "            st.success(\"API key set!\")\n",
        "            api_key = new_api_key\n",
        "\n",
        "    # Build index button\n",
        "    if st.button(\"Build/Rebuild Index\"):\n",
        "        with st.spinner(\"Building index...\"):\n",
        "            st.session_state.index = build_index()\n",
        "            if st.session_state.index:\n",
        "                st.success(\"Index built successfully!\")\n",
        "            else:\n",
        "                st.error(\"Failed to build index\")\n",
        "\n",
        "    # Show data directory contents\n",
        "    st.subheader(\"Data Directory\")\n",
        "    if os.path.exists(DATA_DIR):\n",
        "        files = os.listdir(DATA_DIR)\n",
        "        if files:\n",
        "            st.write(f\"Found {len(files)} files:\")\n",
        "            for file in files:\n",
        "                st.write(f\"- {file}\")\n",
        "        else:\n",
        "            st.write(\"No files found in data directory\")\n",
        "    else:\n",
        "        st.write(\"Data directory does not exist\")\n",
        "\n",
        "    # Clear chat history\n",
        "    if st.button(\"Clear Chat History\"):\n",
        "        st.session_state.messages = []\n",
        "        st.success(\"Chat history cleared\")\n",
        "\n",
        "# Initialize or load index\n",
        "if st.session_state.index is None:\n",
        "    if os.path.exists(CHROMA_DB_DIRECTORY):\n",
        "        with st.spinner(\"Loading existing index...\"):\n",
        "            st.session_state.index = build_index()\n",
        "            if st.session_state.index:\n",
        "                st.success(\"Index loaded successfully!\")\n",
        "    else:\n",
        "        st.info(\"No index found. Please build the index using the sidebar button.\")\n",
        "\n",
        "# Display chat messages\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "        # Show sources if available\n",
        "        if message[\"role\"] == \"assistant\" and \"sources\" in message:\n",
        "            with st.expander(\"View Sources\"):\n",
        "                for i, source in enumerate(message[\"sources\"], 1):\n",
        "                    source_name = source.get('metadata', {}).get('source', f\"Source {i}\")\n",
        "                    st.markdown(f\"**{source_name}**\")\n",
        "                    text = source.get('text', '')\n",
        "                    if text:\n",
        "                        st.text(text[:200] + \"...\" if len(text) > 200 else text)\n",
        "\n",
        "# Chat input\n",
        "if prompt := st.chat_input(\"Ask me anything...\"):\n",
        "    # Check if API key is set\n",
        "    if not api_key:\n",
        "        st.error(\"Please set your Gemini API key in the sidebar first.\")\n",
        "        st.stop()\n",
        "\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    # Display user message\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    # Generate and display assistant response\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            if st.session_state.index is None:\n",
        "                response_content = \"Please build the index first using the sidebar button.\"\n",
        "                st.markdown(response_content)\n",
        "                st.session_state.messages.append({\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": response_content\n",
        "                })\n",
        "            else:\n",
        "                try:\n",
        "                    response = query_index(prompt)\n",
        "                    st.markdown(response[\"answer\"])\n",
        "\n",
        "                    # Add assistant message to chat history\n",
        "                    st.session_state.messages.append({\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": response[\"answer\"],\n",
        "                        \"sources\": response[\"sources\"]\n",
        "                    })\n",
        "\n",
        "                    # Show sources if available\n",
        "                    if response[\"sources\"]:\n",
        "                        with st.expander(\"View Sources\"):\n",
        "                            for i, source in enumerate(response[\"sources\"], 1):\n",
        "                                source_name = source.get('metadata', {}).get('source', f\"Source {i}\")\n",
        "                                st.markdown(f\"**{source_name}**\")\n",
        "                                text = source.get('text', '')\n",
        "                                if text:\n",
        "                                    st.text(text[:200] + \"...\" if len(text) > 200 else text)\n",
        "                except Exception as e:\n",
        "                    error_msg = f\"Error: {str(e)}\"\n",
        "                    st.error(error_msg)\n",
        "                    st.session_state.messages.append({\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": error_msg\n",
        "                    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c61ea93",
      "metadata": {
        "id": "1c61ea93"
      },
      "source": [
        "## Step 8: Create the Streamlit interface for Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "037a342d",
      "metadata": {
        "id": "037a342d"
      },
      "outputs": [],
      "source": [
        "%%writefile colab_streamlit_app.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Streamlit web interface for the chatbot - Google Colab version\n",
        "\"\"\"\n",
        "import streamlit as st\n",
        "import logging\n",
        "import sys\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "from dotenv import load_dotenv\n",
        "import chromadb\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Directory paths\n",
        "DATA_DIR = \"./data\"\n",
        "CHROMA_DB_DIRECTORY = \"./chroma_db\"\n",
        "\n",
        "class GeminiChatbot:\n",
        "    def __init__(self):\n",
        "        # Get Gemini API key\n",
        "        self.api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            print(\"GEMINI_API_KEY not found in environment. Please set it first.\")\n",
        "            self.api_key = st.text_input(\"Enter your Gemini API key:\", type=\"password\")\n",
        "            if not self.api_key:\n",
        "                st.error(\"Gemini API key is required to continue.\")\n",
        "                st.stop()\n",
        "            os.environ[\"GEMINI_API_KEY\"] = self.api_key\n",
        "\n",
        "        # Configure Gemini\n",
        "        genai.configure(api_key=self.api_key)\n",
        "        self.model = genai.GenerativeModel('gemini-pro')\n",
        "        self.index = None\n",
        "\n",
        "    def load_documents(self):\n",
        "        \"\"\"Load documents from the data directory\"\"\"\n",
        "        if not os.path.exists(DATA_DIR):\n",
        "            os.makedirs(DATA_DIR)\n",
        "            st.info(f\"Created data directory at {DATA_DIR}\")\n",
        "\n",
        "        if not os.listdir(DATA_DIR):\n",
        "            st.warning(\"Data directory is empty. Please add some documents.\")\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def build_index(self):\n",
        "        \"\"\"Build vector index from documents\"\"\"\n",
        "        try:\n",
        "            # Dynamic imports to handle different versions of llama-index\n",
        "            try:\n",
        "                # Try importing from llama_index (newer versions)\n",
        "                from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
        "                from llama_index.node_parser import SentenceSplitter\n",
        "                from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "                st.info(\"Using llama_index package\")\n",
        "            except ImportError:\n",
        "                # Try importing from llama_index.core (older or different versions)\n",
        "                from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
        "                from llama_index.core.node_parser import SentenceSplitter\n",
        "                try:\n",
        "                    from llama_index.core.vector_stores.chroma import ChromaVectorStore\n",
        "                    st.info(\"Using llama_index.core package\")\n",
        "                except ImportError:\n",
        "                    from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "                    st.info(\"Using mixed llama_index imports\")\n",
        "\n",
        "            # Load documents\n",
        "            documents = SimpleDirectoryReader(DATA_DIR).load_data()\n",
        "\n",
        "            # Create sentence splitter for text chunking\n",
        "            text_splitter = SentenceSplitter(\n",
        "                chunk_size=1000,\n",
        "                chunk_overlap=200\n",
        "            )\n",
        "\n",
        "            # Set up Chroma client\n",
        "            if not os.path.exists(CHROMA_DB_DIRECTORY):\n",
        "                os.makedirs(CHROMA_DB_DIRECTORY)\n",
        "\n",
        "            chroma_client = chromadb.PersistentClient(path=CHROMA_DB_DIRECTORY)\n",
        "            chroma_collection = chroma_client.get_or_create_collection(\"documents\")\n",
        "            vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "            storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "            # Build index with error handling for different parameter names\n",
        "            try:\n",
        "                self.index = VectorStoreIndex.from_documents(\n",
        "                    documents,\n",
        "                    storage_context=storage_context,\n",
        "                    transformations=[text_splitter],\n",
        "                )\n",
        "            except TypeError:\n",
        "                # Try alternative approach if transformations parameter doesn't work\n",
        "                self.index = VectorStoreIndex.from_documents(\n",
        "                    documents,\n",
        "                    storage_context=storage_context,\n",
        "                    node_parser=text_splitter,\n",
        "                )\n",
        "\n",
        "            st.success(\"Index built successfully!\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error building index: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return False\n",
        "\n",
        "    def query(self, query_text):\n",
        "        \"\"\"Query the index with a natural language query\"\"\"\n",
        "        if not self.index:\n",
        "            st.error(\"Index not built yet. Please build the index first.\")\n",
        "            return {\n",
        "                \"answer\": \"Index not built yet. Please build the index first.\",\n",
        "                \"sources\": []\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            # Create query engine\n",
        "            query_engine = self.index.as_query_engine()\n",
        "\n",
        "            # Execute query\n",
        "            response = query_engine.query(query_text)\n",
        "\n",
        "            # Extract source information\n",
        "            sources = []\n",
        "            if hasattr(response, 'source_nodes'):\n",
        "                for node in response.source_nodes:\n",
        "                    source = {\n",
        "                        \"text\": node.node.get_content(),\n",
        "                        \"metadata\": node.node.metadata,\n",
        "                        \"score\": node.score if hasattr(node, 'score') else None\n",
        "                    }\n",
        "                    sources.append(source)\n",
        "\n",
        "            return {\n",
        "                \"answer\": str(response),\n",
        "                \"sources\": sources\n",
        "            }\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error during query: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return {\n",
        "                \"answer\": f\"Error during query: {str(e)}\",\n",
        "                \"sources\": []\n",
        "            }\n",
        "\n",
        "def init_session_state():\n",
        "    \"\"\"Initialize session state variables\"\"\"\n",
        "    if 'messages' not in st.session_state:\n",
        "        st.session_state.messages = []\n",
        "    if 'chatbot' not in st.session_state:\n",
        "        st.session_state.chatbot = None\n",
        "\n",
        "def main():\n",
        "    # Set page config\n",
        "    st.set_page_config(\n",
        "        page_title=\"Gemini Chatbot for Google Colab\",\n",
        "        page_icon=\"🤖\",\n",
        "        layout=\"wide\"\n",
        "    )\n",
        "\n",
        "    # Initialize session state\n",
        "    init_session_state()\n",
        "\n",
        "    # Main title\n",
        "    st.title(\"🤖 Gemini Chatbot for Google Colab\")\n",
        "\n",
        "    # Sidebar\n",
        "    with st.sidebar:\n",
        "        st.title(\"Chatbot Settings\")\n",
        "\n",
        "        # Initialize chatbot if not already done\n",
        "        if st.session_state.chatbot is None:\n",
        "            st.session_state.chatbot = GeminiChatbot()\n",
        "\n",
        "        # Check data directory status\n",
        "        data_dir_status = \"✅ Available\" if os.path.exists(DATA_DIR) and os.listdir(DATA_DIR) else \"❌ Empty\"\n",
        "        st.info(f\"Data Directory: {data_dir_status}\")\n",
        "\n",
        "        # Check index status\n",
        "        index_status = \"✅ Available\" if os.path.exists(CHROMA_DB_DIRECTORY) else \"❌ Not Built\"\n",
        "        st.info(f\"Vector Index: {index_status}\")\n",
        "\n",
        "        # Upload documents\n",
        "        with st.expander(\"Upload Documents\"):\n",
        "            uploaded_file = st.file_uploader(\"Upload a document\", type=[\"txt\", \"md\", \"pdf\"])\n",
        "            if uploaded_file is not None:\n",
        "                if not os.path.exists(DATA_DIR):\n",
        "                    os.makedirs(DATA_DIR)\n",
        "\n",
        "                file_path = os.path.join(DATA_DIR, uploaded_file.name)\n",
        "                with open(file_path, \"wb\") as f:\n",
        "                    f.write(uploaded_file.getvalue())\n",
        "                st.success(f\"File '{uploaded_file.name}' uploaded successfully\")\n",
        "\n",
        "        # Build index button\n",
        "        if st.button(\"Build/Rebuild Vector Index\"):\n",
        "            if st.session_state.chatbot.load_documents():\n",
        "                with st.spinner(\"Building vector index...\"):\n",
        "                    if st.session_state.chatbot.build_index():\n",
        "                        st.success(\"Vector index built successfully!\")\n",
        "                    else:\n",
        "                        st.error(\"Failed to build vector index.\")\n",
        "            else:\n",
        "                st.error(\"No documents found. Please upload some documents first.\")\n",
        "\n",
        "    # Display chat messages\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.markdown(message[\"content\"])\n",
        "\n",
        "            # Show sources if available\n",
        "            if message[\"role\"] == \"assistant\" and \"sources\" in message and message[\"sources\"]:\n",
        "                with st.expander(\"View Sources\"):\n",
        "                    for i, source in enumerate(message[\"sources\"], 1):\n",
        "                        source_name = source.get('metadata', {}).get('source', f\"Source {i}\")\n",
        "                        st.markdown(f\"**Document: {source_name}**\")\n",
        "                        text = source.get('text', '')\n",
        "                        if text:\n",
        "                            st.text(text[:200] + \"...\" if len(text) > 200 else text)\n",
        "\n",
        "    # Chat input\n",
        "    if prompt := st.chat_input(\"Ask me anything...\"):\n",
        "        # Add user message to chat history\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "        # Display user message\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.markdown(prompt)\n",
        "\n",
        "        # Check if index is built\n",
        "        if not st.session_state.chatbot.index:\n",
        "            if not os.path.exists(CHROMA_DB_DIRECTORY) or not os.listdir(CHROMA_DB_DIRECTORY):\n",
        "                # Need to build index first\n",
        "                if st.session_state.chatbot.load_documents():\n",
        "                    with st.spinner(\"Building index for the first time...\"):\n",
        "                        st.session_state.chatbot.build_index()\n",
        "                else:\n",
        "                    st.error(\"No documents found. Please upload some documents first.\")\n",
        "                    st.session_state.messages.append({\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": \"I need some documents to learn from. Please upload documents using the sidebar.\",\n",
        "                        \"sources\": []\n",
        "                    })\n",
        "                    st.stop()\n",
        "            else:\n",
        "                # Index exists but not loaded\n",
        "                with st.spinner(\"Loading existing index...\"):\n",
        "                    st.session_state.chatbot.build_index()\n",
        "\n",
        "        # Generate and display assistant response\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            message_placeholder = st.empty()\n",
        "            with st.spinner(\"Thinking...\"):\n",
        "                response = st.session_state.chatbot.query(prompt)\n",
        "                message_placeholder.markdown(response[\"answer\"])\n",
        "\n",
        "                # Add assistant message to chat history\n",
        "                st.session_state.messages.append({\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": response[\"answer\"],\n",
        "                    \"sources\": response[\"sources\"]\n",
        "                })\n",
        "\n",
        "                # Show sources if available\n",
        "                if response[\"sources\"]:\n",
        "                    with st.expander(\"View Sources\"):\n",
        "                        for i, source in enumerate(response[\"sources\"], 1):\n",
        "                            source_name = source.get('metadata', {}).get('source', f\"Source {i}\")\n",
        "                            st.markdown(f\"**Document: {source_name}**\")\n",
        "                            text = source.get('text', '')\n",
        "                            if text:\n",
        "                                st.text(text[:200] + \"...\" if len(text) > 200 else text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf55f626",
      "metadata": {
        "id": "bf55f626"
      },
      "source": [
        "## Step 9: Run Streamlit Interface (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52d5a9cb",
      "metadata": {
        "id": "52d5a9cb"
      },
      "outputs": [],
      "source": [
        "# Install ngrok and pyngrok if you want to run the Streamlit app\n",
        "!pip install pyngrok\n",
        "\n",
        "# Run Streamlit app using ngrok\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "\n",
        "# Set up ngrok authentication (optional, needed for better reliability)\n",
        "# Get your token from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "NGROK_AUTH_TOKEN = input(\"Enter your ngrok auth token (optional, press Enter to skip): \")\n",
        "if NGROK_AUTH_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Start ngrok tunnel to Streamlit\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"\\n\\nStreamlit app is available at: {public_url}\\n\\n\")\n",
        "\n",
        "# Run Streamlit app\n",
        "!streamlit run colab_streamlit_app.py &"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b47cce1",
      "metadata": {
        "id": "1b47cce1"
      },
      "source": [
        "## Step 10: Clean up (Run this when you're done)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed325e96",
      "metadata": {
        "id": "ed325e96"
      },
      "outputs": [],
      "source": [
        "# Clean up\n",
        "!pkill -f streamlit  # Stop Streamlit if running\n",
        "ngrok.kill()  # Stop ngrok if running"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}